```
scipy==1.0.0
numpy==1.14.0
Pillow==8.1.1
tabulate==0.8.2
tensorflow==1.7.0rc0
torch==0.3.0.post4
torchvision==0.2.0
tqdm==4.19.8
torchnet
```
该代码的库版本过低，实在找不到支持torch为0.3.0版本的环境，所以只分析了代码，分析利用从预训练的ResNet-18模型中提炼的知识训练5层CNN。
# 加载参数
首先在命令行中输入：
```Bash
python train.py --model_dir experiments/cnn_distill
```
在该模型目录中加载`params.json`文件，路径设定为`json_path`。
通过`utils`文件中的`Params`类加载超参数，得到以下结果：
```json
{
    "model_version": "cnn_distill",
    "subset_percent": 1.0,
    "augmentation": "yes",
    "teacher": "resnet18",
    "alpha": 0.9,
    "temperature": 20,
    "learning_rate": 1e-3,
    "batch_size": 128,
    "num_epochs": 30,
    "dropout_rate": 0.5, 
    "num_channels": 32,
    "save_summary_steps": 100,
    "num_workers": 4
}
```
# 加载模型
在该部分，根据`Params`的参数来加载不同的模型，以从预训练的ResNet-18模型中提炼的知识训练5层CNN为例，首先加载net文件中的`Net`类，该类将创建一个5层的CNN模型，使用Adam优化器：
```Python
if params.model_version == "cnn_distill":
	model = net.Net(params).cuda() if params.cuda else net.Net(params)
	optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)
	# fetch loss function and metrics definition in model files
	loss_fn_kd = net.loss_fn_kd
	metrics = net.metrics
```
> 知识蒸馏的loss计算：
> 教师网络的softmax输出需要经过升温，预测结果称为软标签`soft label`。学生网络在softmax输出结果前有两个分支：
> 1. 一个分支也需要进行升温，在预测的时候得到软预测`soft prediction`，然后对`soft label`和`soft predictions` 计算损失函数，称为`distillation loss`。
> 2. 另一个分支不需要进行升温，直接进行softmax，此时的预测结果称为`hard prediction`，然后`hard prediction`与label直接计算损失，称为`student loss`。
> 这两种损失通过加权组合在一起：$$Loss=(1-\alpha)L^{KD}_{`distillation loss`}+\alpha L^{KD}_{student loss}$$

此处的使用的`distillation loss`是torch的`KLDivLoss`，`student loss`使用的是交叉熵。由于`KLDivLoss`损失计算函数要求：![Pasted image 20240831090207](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240831090207.png)
所以在代码中，学生模型的输出使用`log_softmax`，而教师模型的输出使用`softmax`：
```Python
def loss_fn_kd(outputs, labels, teacher_outputs, params):
    alpha = params.alpha
    T = params.temperature
    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + F.cross_entropy(outputs, labels) * (1. - alpha)
    return KD_loss
```

创建教师模型时，调用resnet文件中的`ResNet18`函数，该函数将返回一个resnet类，该类将创建Resnet18，ResNet-18是指ResNet网络的第18个层，它包含18个残差块。每个残差块都包含一个卷积层和一个批归化层（Batch Normalization），并通过残差连接将输入和输出相加。这样可以使网络能够训练更深的层次，而不容易出现梯度消失或爆炸的问题。
```Python
if params.teacher == "resnet18":
	teacher_model = resnet.ResNet18()
	teacher_checkpoint = 'experiments/base_resnet18/best.pth.tar'
	teacher_model = teacher_model.cuda() if params.cuda else teacher_model
```
# 训练
在`train_and_evaluate_kd`函数中进行训练。
如果模型参数`restore_file`不为空，则从指定的文件路径加载模型权重和优化器状态。
```Python
if restore_file is not None:
	restore_path = os.path.join(args.model_dir, args.restore_file + '.pth.tar')
	logging.info("Restoring parameters from {}".format(restore_path))
	utils.load_checkpoint(restore_path, model, optimizer)
```
创建一个学习率调度器，用于在训练过程中动态调整优化器的学习率。
```Python
scheduler = StepLR(optimizer, step_size=100, gamma=0.2)
```
## `train_kd`函数
在每一轮的训练中，都将调用`train_kd`函数。
在该函数中，首先将模型转为`train`模式：`model.train()`。
从dataloader中读取样本和标签，将样本依次传入给学生模型和教师模型，将二者的输出和label传入loss函数计算loss：
```Python
output_batch = model(train_batch)
...
output_teacher_batch = output_teacher_batch.cuda(async=True)
...
loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch, params)
```
## `evaluate_kd`函数
每一轮训练结束后，都需要在验证集上进行评估，在该函数中只会用到学生模型，与训练时类似

每一轮的训练和验证完成后，都需要保存checkpoint，并保存最好的参数：
```Python
# Save weights
utils.save_checkpoint({'epoch': epoch + 1,
					   'state_dict': model.state_dict(),
					   'optim_dict' : optimizer.state_dict()},
					   is_best=is_best,
					   checkpoint=model_dir)
# If best_eval, best_save_path
if is_best:
	logging.info("- Found new best accuracy")
	best_val_acc = val_acc
	# Save best val metrics in a json file in the model directory
	best_json_path = os.path.join(model_dir, "metrics_val_best_weights.json")
	utils.save_dict_to_json(val_metrics, best_json_path)
# Save latest val metrics in a json file in the model directory
last_json_path = os.path.join(model_dir, "metrics_val_last_weights.json")
utils.save_dict_to_json(val_metrics, last_json_path)
```