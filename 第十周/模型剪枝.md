# 使用剪枝后模型推理
首先使用`pipeline`导入模型：
```Python
MODEL_NAME = "madlag/bert-base-uncased-squadv1-x2.44-f87.7-d26-hybrid-filled-v1"
qa_pipeline = pipeline("question-answering",model=MODEL_NAME,device=0)
```
通过`nn_pruning`的`optimize_model`来对模型进行剪枝，参数`dense`指定了优化方法为dense（密集），通常意味着对模型的某些稀疏部分进行优化，使得它们更紧凑或高效，优化后的模型将覆盖原有的qa_pipeline.model。
这个过程可能包括：
1. 减少模型的冗余参数。
2. 通过将稀疏的结构转换为更紧凑的形式，来提升推理速度。
3. 减少模型的内存占用。

通过打印模型参数对比剪枝效果：
```
BERT-base parameters: 110.0M
...
Parameters count after optimization=46.2M
Reduction of the total number of parameters compared to BERT-base:2.38X
```
以第一个被优化的层为例，原始的输出为3072，优化后为552，优化了82.03%：
![Pasted image 20240827212525](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240827212525.png)

接下来将打印BERT模型中各层的参数大小。它遍历模型的参数，根据参数名称确定其所在的层和类型（权重或偏置），然后打印出该参数的理论大小和实际大小。输出结果：
```
BERT-base Size, Model Size, Layer Name

Layer 0
[768, 768] => [256, 768], attention.self.query.weight
[768] => [256], attention.self.query.bias
[768, 768] => [256, 768], attention.self.key.weight
[768] => [256], attention.self.key.bias
[768, 768] => [256, 768], attention.self.value.weight
[768] => [256], attention.self.value.bias
[768, 768] => [768, 256], attention.output.dense.weight
[768] => [768], attention.output.dense.bias
[3072, 768] => [552, 768], intermediate.dense.weight
[768] => [552], intermediate.dense.bias
[768, 3072] => [768, 552], output.dense.weight
[768] => [768], output.dense.bias
...
```
# 使用稀疏训练器进行fine-pruning
由于该文件的模型较大，需要在服务器上运行，所以在下载huggingface模型时，需要配置镜像源：
```Python
import os
os.environ['HF_ENDPOINT']='https://hf-mirror.com'
```
## 导入数据集
此次使用的数据集是BoolQ dataset，通过`datasets`库导入：
```Python
from datasets import load_dataset
boolq = load_dataset("super_glue", "boolq")
boolq['train'][0]
# output:
# {'question': 'do iran and afghanistan speak the same language', 'passage': 'Persian language -- Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.', 'idx': 0, 'label': 1}
```
为了让trainer自动识别到label，需要将label重命名为`labels`：
```Python
boolq.rename_column("label", "labels")
```
**注**：原始文件的方法`rename_column_`已废弃，更改为`rename_column`使用。
## Tokenization
tokenizer使用`"bert-base-uncased"`，对数据集的`question`和`passage`进行tokenize：
```Python
from transformers import AutoTokenizer
bert_ckpt = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(bert_ckpt)
def tokenize_and_encode(examples):
    return tokenizer(examples['question'], examples['passage'], truncation="only_second")
boolq_enc = boolq.map(tokenize_and_encode, batched=True)
```
## Trainer
接下来创建一个trainer，它可以为我们处理精细剪枝和评估步骤。在`nn_pruning`中，这是通过`sparse_trainer.SparseTrainer`完成的，它为`transformers.Trainer`提供了额外的方法，用于 "修补"或稀疏化预训练模型，并实现运动剪枝论文中讨论的各种剪枝技术。为了简单起见，需要重写loss函数，忽略知识蒸馏，只返回交叉熵损失：
```Python
from transformers import Trainer
from nn_pruning.sparse_trainer import SparseTrainer

class PruningTrainer(SparseTrainer, Trainer):
    def __init__(self, sparse_args, *args, **kwargs):
        Trainer.__init__(self, *args, **kwargs)
        SparseTrainer.__init__(self, sparse_args)
        
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        We override the default loss in SparseTrainer because it throws an 
        error when run without distillation
        """
        outputs = model(**inputs)
        if self.args.past_index >= 0:
            self._past = outputs[self.args.past_index]

        loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]
        self.metrics["ce_loss"] += float(loss)
        self.loss_counter += 1
        return (loss, outputs) if return_outputs else loss
```
然后创建`PruningTrainer`需要的`sparse_args`：
```Python
from nn_pruning.patch_coordinator import SparseTrainingArguments
sparse_args = SparseTrainingArguments()
```
接着设计相关的超参数和参数，需要注意的是，在模型checkpoints保存时会出现共享张量的问题，需要设置不保存checkpoints：
```Python
args = TrainingArguments(
    output_dir="checkpoints",
    evaluation_strategy="epoch",
    save_strategy="no",
    ...
)
```
## Patching a Dense Model
Patching a Dense Model（对密集模型进行补丁）是指在不更改模型原始架构的情况下，动态修改或增强一个已经训练好的密集（非稀疏）神经网络模型，以实现某种特定的功能或优化效果。这通常涉及应用稀疏化技术，如剪枝、量化等。
1. 密集模型（Dense Model）：
   - 密集模型是指所有的神经网络层（通常是线性层、卷积层等）都包含了全部的参数权重，模型没有进行任何形式的稀疏化或剪枝。因此，所有的神经元和连接都是被使用的，没有被丢弃。
   - 这种模型往往具有良好的性能和预测能力，但在计算和存储上是昂贵的，特别是在大规模应用或资源受限的环境中（如移动设备或嵌入式系统）。
2. 补丁（Patching）：
   - 对一个密集模型进行补丁，即在保持其架构不变的情况下，通过软件或轻量级的修改将稀疏化机制嵌入到模型中，以实现优化。
   - 这通常在模型的训练阶段或推理阶段（inference）中进行。
3. 稀疏化技术：
   - 剪枝（Pruning）： 删除模型中不重要的权重或神经元，使得模型更加稀疏，从而减少计算和存储开销。
4. Patching的用途：
   - 提升效率： 在不重新设计或训练模型的前提下，提升计算效率或减少资源消耗。
   - 保持性能： 应用补丁后，尽可能保留模型的原始预测性能。
   - 动态适应性： 在不同的设备或场景中，可以快速地对模型进行调整以适应特定的硬件环境或任务需求。
```Python
import torch 
from transformers import AutoModelForSequenceClassification
from nn_pruning.patch_coordinator import ModelPatchingCoordinator

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

mpc = ModelPatchingCoordinator(
    sparse_args=sparse_args, 
    model_name_or_path=bert_ckpt,
    device=device, 
    cache_dir="checkpoints", 
    logit_names="logits", 
    teacher_constructor=None)
bert_model = AutoModelForSequenceClassification.from_pretrained(bert_ckpt).to(device)
mpc.patch_model(bert_model)

bert_model.save_pretrained("models/patched",safe_serialization=False)
```
`ModelPatchingCoordinator` 是用来协调稀疏训练过程的核心类。它将稀疏化技术（如修剪和量化）应用到模型上。使用 `ModelPatchingCoordinator` 对一个预训练的序列分类模型（例如 BERT）进行稀疏化训练。具体步骤：
1. 初始化稀疏化参数和配置。
2. 加载一个预训练的 BERT 模型，并准备对其应用稀疏化技术。
3. 设置训练设备和缓存目录。
4. 确定模型输出（logits）名称，以及是否需要使用蒸馏技术（教师模型）。
> **注意**：要保存模型的时候会出现共享张量的问题，此处使用`safe_serialization=False` 解决问题。

## Fine-pruning
在该部分进行正式的模型训练，首先配置好trainer的各项参数：
```Python
trainer = PruningTrainer(
    sparse_args=sparse_args,
    args=args,
    model=bert_model,
    train_dataset=boolq_enc["train"],
    eval_dataset=boolq_enc["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```
然后通过`trainer.train()`训练模型：![Snipaste_2024-08-29_10-07-14](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Snipaste_2024-08-29_10-07-14.png)
## Optimising for inference
通过下列代码来查看两个特定权重矩阵权重稀疏性:
```Python
from matplotlib import pyplot as plt

parameters = dict(trainer.model.named_parameters())
param_names = ["bert.encoder.layer.11.intermediate.dense.weight",
               "bert.encoder.layer.11.attention.output.dense.weight"]

for param_name in param_names:          
    w = parameters[param_name]
    print(param_name)

    plt.imshow((w != 0).detach().cpu())
    plt.show()
```
结果如下：
![Pasted image 20240829180607](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240829180607.png)
![Pasted image 20240829180621](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240829180621.png)
然而，这并不会在推理过程中给我们带来任何加速，因为矩阵乘法并不会因为更多的值为零而变得更快。为了解决这个问题，`nn_pruning`提供了一个`optimize_model`函数，它将巧妙地从模型中删除零，并产生一个参数更少的修剪模型(因此推理速度更快)：
```Python
from nn_pruning.inference_model_patcher import optimize_model
prunebert_model = optimize_model(trainer.model, "dense")
```
为了了解剪枝模型能带来怎样的推理收益，构建一个函数从涉及固定问题-段落对的多次运行中计算平均延迟：
```Python
from time import perf_counter

def compute_latencies(model,
                      question="Is Saving Private Ryan based on a book?",
                      passage="""In 1994, Robert Rodat wrote the script for the film. Rodat’s script was submitted to 
                      producer Mark Gordon, who liked it and in turn passed it along to Spielberg to direct. The film is 
                      loosely based on the World War II life stories of the Niland brothers. A shooting date was set for 
                      June 27, 1997"""):
    inputs = tokenizer(question, passage, truncation="only_second", return_tensors="pt")
    latencies = []
    
    # Warmup
    for _ in range(10):
        _ = model(**inputs)
        
    for _ in range(100):
        start_time = perf_counter()
        _ = model(**inputs)
        latency = perf_counter() - start_time 
        latencies.append(latency)
        # Compute run statistics
        time_avg_ms = 1000 * np.mean(latencies)
        time_std_ms = 1000 * np.std(latencies) 
    print(f"Average latency (ms) - {time_avg_ms:.2f} +\- {time_std_ms:.2f}") 
    return {"time_avg_ms": time_avg_ms, "time_std_ms": time_std_ms}
```
分别对剪枝前后的模型计算延迟：
```Python
latencies = {}
latencies["prunebert"] = compute_latencies(prunebert_model.to("cpu"))
bert_unpruned = AutoModelForSequenceClassification.from_pretrained("lewtun/bert-base-uncased-finetuned-boolq").to("cpu")
latencies["bert-base"] = compute_latencies(bert_unpruned.to("cpu"))
```
得到的结果对比：
![Pasted image 20240829181431](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240829181431.png)
可以看到：剪枝后的模型更小，推理速度更快