{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局变量\n",
    "entities2id = {}\n",
    "relations2id = {}\n",
    "relation_tph = {}\n",
    "relation_hpt = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(file1, file2, file3, file4):\n",
    "    print(\"load file...\")\n",
    "\n",
    "    entity = []\n",
    "    relation = []\n",
    "    with open(file2, 'r') as f1, open(file3, 'r') as f2:\n",
    "        lines1 = f1.readlines()\n",
    "        lines2 = f2.readlines()\n",
    "        for line in lines1:\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            entities2id[line[0]] = line[1]\n",
    "            entity.append(int(line[1]))\n",
    "\n",
    "        for line in lines2:\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            relations2id[line[0]] = line[1]\n",
    "            relation.append(int(line[1]))\n",
    "\n",
    "    triple_list = []\n",
    "    relation_head = {} # 关系与头实体的字典集合计数\n",
    "    relation_tail = {} # 关系与尾实体的字典集合计数\n",
    "\n",
    "    with codecs.open(file1, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            triple = line.strip().split(\"\\t\")\n",
    "            if len(triple) != 3:\n",
    "                continue\n",
    "\n",
    "            h_ = int(entities2id[triple[0]])\n",
    "            r_ = int(relations2id[triple[1]])\n",
    "            t_ = int(entities2id[triple[2]])\n",
    "\n",
    "            # 对关系和头实体的集合计数\n",
    "            triple_list.append([h_, r_, t_])\n",
    "            if r_ in relation_head:\n",
    "                if h_ in relation_head[r_]:\n",
    "                    relation_head[r_][h_] += 1\n",
    "                else:\n",
    "                    relation_head[r_][h_] = 1\n",
    "            else:\n",
    "                relation_head[r_] = {}\n",
    "                relation_head[r_][h_] = 1\n",
    "\n",
    "            # 对关系和尾实体的集合计数\n",
    "            if r_ in relation_tail:\n",
    "                if t_ in relation_tail[r_]:\n",
    "                    relation_tail[r_][t_] += 1\n",
    "                else:\n",
    "                    relation_tail[r_][t_] = 1\n",
    "            else:\n",
    "                relation_tail[r_] = {}\n",
    "                relation_tail[r_][t_] = 1\n",
    "    # 计算每个关系平均头实体数\n",
    "    for r_ in relation_head:\n",
    "        sum1, sum2 = 0, 0\n",
    "        for head in relation_head[r_]:\n",
    "            sum1 += 1\n",
    "            sum2 += relation_head[r_][head]\n",
    "        tph = sum2 / sum1\n",
    "        relation_tph[r_] = tph\n",
    "    # 计算每个关系平均尾实体数\n",
    "    for r_ in relation_tail:\n",
    "        sum1, sum2 = 0, 0\n",
    "        for tail in relation_tail[r_]:\n",
    "            sum1 += 1\n",
    "            sum2 += relation_tail[r_][tail]\n",
    "        hpt = sum2 / sum1\n",
    "        relation_hpt[r_] = hpt\n",
    "    # 获得验证集的三元组集合\n",
    "    valid_triple_list = []\n",
    "    with codecs.open(file4, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            triple = line.strip().split(\"\\t\")\n",
    "            if len(triple) != 3:\n",
    "                continue\n",
    "\n",
    "            h_ = int(entities2id[triple[0]])\n",
    "            r_ = int(relations2id[triple[1]])\n",
    "            t_ = int(entities2id[triple[2]])\n",
    "\n",
    "            valid_triple_list.append([h_, r_, t_])\n",
    "\n",
    "    print(\"Complete load. entity : %d , relation : %d , train triple : %d, , valid triple : %d\" % (\n",
    "        len(entity), len(relation), len(triple_list), len(valid_triple_list)))\n",
    "\n",
    "    return entity, relation, triple_list, valid_triple_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_l1(h, r, t):\n",
    "    return np.sum(np.fabs(h + r - t))\n",
    "\n",
    "\n",
    "def norm_l2(h, r, t):\n",
    "    return np.sum(np.square(h + r - t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E(nn.Module):\n",
    "    def __init__(self, entity_num, relation_num, dim, margin, norm, C):\n",
    "        super(E, self).__init__()\n",
    "        self.entity_num = entity_num\n",
    "        self.relation_num = relation_num\n",
    "        self.dim = dim\n",
    "        self.margin = margin\n",
    "        self.norm = norm\n",
    "        self.C = C\n",
    "        # 初始化 实体数量*维度的词嵌入 14951*50  关系数量*维度的词嵌入 1345*50\n",
    "        # nn.Embedding.weight随机初始化方式是标准正态分布\n",
    "        self.ent_embedding = torch.nn.Embedding(num_embeddings=self.entity_num,\n",
    "                                                embedding_dim=self.dim).cuda()\n",
    "        self.rel_embedding = torch.nn.Embedding(num_embeddings=self.relation_num,\n",
    "                                                embedding_dim=self.dim).cuda()\n",
    "\n",
    "        self.loss_F = nn.MarginRankingLoss(self.margin, reduction=\"mean\").cuda()  # 计算两个向量之间的相似度，当两个向量之间的距离大于margin，则loss为正，小于margin，loss为0\n",
    "\n",
    "        self.__data_init()\n",
    "\n",
    "    def __data_init(self):\n",
    "        nn.init.xavier_uniform_(self.ent_embedding.weight.data) # Xavier 初始化\n",
    "        nn.init.xavier_uniform_(self.rel_embedding.weight.data) # Xavier 初始化\n",
    "        self.normalization_rel_embedding()\n",
    "        self.normalization_ent_embedding()\n",
    "\n",
    "    def normalization_ent_embedding(self):\n",
    "        norm = self.ent_embedding.weight.detach().cpu().numpy() # 转换成numpy的格式\n",
    "        # norm:(14951,50)  norm/(norm平方加起来再开平方)  axis=1是对1这个维度进行计算，也就是保留14951的维度\n",
    "        # 为了能做除法，所以keepdims=True\n",
    "        # 这一步的分母是在计算L2范数，进行L2范数归一化\n",
    "        norm_weight = np.sqrt(np.sum(np.square(norm), axis=1, keepdims=True))\n",
    "        # norm_weight:(14951,1)\n",
    "        norm = norm / norm_weight\n",
    "\n",
    "        self.ent_embedding.weight.data.copy_(torch.from_numpy(norm))\n",
    "\n",
    "    def normalization_rel_embedding(self):\n",
    "        norm = self.rel_embedding.weight.detach().cpu().numpy()\n",
    "        norm = norm / np.sqrt(np.sum(np.square(norm), axis=1, keepdims=True))\n",
    "        self.rel_embedding.weight.data.copy_(torch.from_numpy(norm))  # 把词向量矩阵作为参数复制过去\n",
    "\n",
    "    def input_pre_transe(self, ent_vector, rel_vector):\n",
    "        for i in range(self.entity_num):\n",
    "            self.ent_embedding.weight.data[i] = torch.from_numpy(np.array(ent_vector[i]))\n",
    "        for i in range(self.relation_num):\n",
    "            self.rel_embedding.weight.data[i] = torch.from_numpy(np.array(rel_vector[i]))\n",
    "\n",
    "    def distance(self, h, r, t):\n",
    "        head = self.ent_embedding(h)\n",
    "        rel = self.rel_embedding(r)\n",
    "        tail = self.ent_embedding(t)\n",
    "\n",
    "        distance = head + rel - tail\n",
    "        # self.norm = 1就是一范数    dim=1：指定计算的维度   distance:(9600,50)\n",
    "        score = torch.norm(distance, p=self.norm, dim=1) # torch.norm返回所给tensor的矩阵范数或向量范数\n",
    "        return score\n",
    "\n",
    "    def test_distance(self, h, r, t):\n",
    "\n",
    "        head = self.ent_embedding(h.cuda()) # 找到对应的向量\n",
    "        rel = self.rel_embedding(r.cuda())\n",
    "        tail = self.ent_embedding(t.cuda())\n",
    "\n",
    "        distance = head + rel - tail\n",
    "\n",
    "        score = torch.norm(distance, p=self.norm, dim=1)\n",
    "        return score.cpu().detach().numpy()\n",
    "\n",
    "    def scale_loss(self, embedding):\n",
    "        return torch.sum(\n",
    "            torch.max(\n",
    "                torch.sum(\n",
    "                    embedding ** 2, dim=1, keepdim=True\n",
    "                ) - torch.autograd.Variable(torch.FloatTensor([1.0]).cuda()),\n",
    "                torch.autograd.Variable(torch.FloatTensor([0.0]).cuda())\n",
    "            ))\n",
    "\n",
    "    # 计算loss\n",
    "    def forward(self, current_triples, corrupted_triples):\n",
    "        h, r, t = torch.chunk(current_triples, 3, dim=1)  # chunk方法可以对张量分块，返回一个张量列表\n",
    "        h_c, r_c, t_c = torch.chunk(corrupted_triples, 3, dim=1)\n",
    "\n",
    "        h = torch.squeeze(h, dim=1).cuda() # squeeze()函数的功能是维度压缩。返回一个tensor（张量），其中input中大小为1的所有维都已删除\n",
    "        r = torch.squeeze(r, dim=1).cuda()\n",
    "        t = torch.squeeze(t, dim=1).cuda()\n",
    "        h_c = torch.squeeze(h_c, dim=1).cuda()\n",
    "        r_c = torch.squeeze(r_c, dim=1).cuda()\n",
    "        t_c = torch.squeeze(t_c, dim=1).cuda()\n",
    "\n",
    "        # pos:正确（头+关系-尾）的一范数    错误 neg:（头+关系-尾）的一范数\n",
    "        pos = self.distance(h, r, t)\n",
    "        neg = self.distance(h_c, r_c, t_c)\n",
    "\n",
    "        entity_embedding = self.ent_embedding(torch.cat([h, t, h_c, t_c]).cuda()) # 实体嵌入\n",
    "        relation_embedding = self.rel_embedding(torch.cat([r, r_c]).cuda())  # 关系嵌入\n",
    "\n",
    "        y = Variable(torch.Tensor([-1])).cuda()\n",
    "        # loss 计算正负三元组的距离，也就是（r+(d+)-(d-)）\n",
    "        loss = self.loss_F(pos, neg, y)  # self.loss_F = nn.MarginRankingLoss(self.margin, reduction=\"mean\").cuda()\n",
    "\n",
    "        ent_scale_loss = self.scale_loss(entity_embedding) # 不懂这个是干什么的\n",
    "        rel_scale_loss = self.scale_loss(relation_embedding)\n",
    "        return loss + self.C * (ent_scale_loss / len(entity_embedding) + rel_scale_loss / len(relation_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE:\n",
    "    def __init__(self, entity, relation, triple_list, embedding_dim=50, lr=0.01, margin=1.0, norm=1, C=1.0,\n",
    "                 valid_triple_list=None):\n",
    "        self.entities = entity\n",
    "        self.relations = relation\n",
    "        self.triples = triple_list\n",
    "        self.dimension = embedding_dim\n",
    "        self.learning_rate = lr\n",
    "        self.margin = margin\n",
    "        self.norm = norm\n",
    "        self.loss = 0.0\n",
    "        self.valid_loss = 0.0\n",
    "        self.valid_triples = valid_triple_list\n",
    "        self.train_loss = []\n",
    "        self.validation_loss = []\n",
    "\n",
    "        self.test_triples = []\n",
    "\n",
    "        self.C = C\n",
    "\n",
    "    def data_initialise(self):\n",
    "        # 创建一个模型,把向量放到里面作为优化的参数\n",
    "        self.model = E(len(self.entities), len(self.relations), self.dimension, self.margin, self.norm, self.C)\n",
    "        self.optim = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # self.optim = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def insert_pre_data(self, file1, file2):\n",
    "        entity_dic = {}\n",
    "        relation = {}\n",
    "        with codecs.open(file1, 'r') as f1, codecs.open(file2, 'r') as f2:\n",
    "            lines1 = f1.readlines()\n",
    "            lines2 = f2.readlines()\n",
    "            for line in lines1:\n",
    "                line = line.strip().split('\\t')\n",
    "                if len(line) != 2:\n",
    "                    continue\n",
    "                entity_dic[int(line[0])] = json.loads(line[1])\n",
    "\n",
    "            for line in lines2:\n",
    "                line = line.strip().split('\\t')\n",
    "                if len(line) != 2:\n",
    "                    continue\n",
    "                relation[int(line[0])] = json.loads(line[1])\n",
    "\n",
    "        self.model.input_pre_transe(entity_dic, relation)\n",
    "\n",
    "    def insert_test_data(self, file1, file2, file3):\n",
    "        self.insert_pre_data(file1, file2)\n",
    "\n",
    "        triple_list = []\n",
    "        with codecs.open(file3, 'r') as f4:\n",
    "            content = f4.readlines()\n",
    "            for line in content:\n",
    "                triple = line.strip().split(\"\\t\")\n",
    "                if len(triple) != 3:\n",
    "                    continue\n",
    "\n",
    "                head = int(entities2id[triple[0]])\n",
    "                relation = int(relations2id[triple[1]])\n",
    "                tail = int(entities2id[triple[2]])\n",
    "\n",
    "                triple_list.append([head, relation, tail])\n",
    "\n",
    "        self.test_triples = triple_list\n",
    "\n",
    "    def insert_traning_data(self, file1, file2, file3):\n",
    "        self.insert_pre_data(file1, file2)\n",
    "        with codecs.open(file3, 'r') as f5:\n",
    "            lines = f5.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip().split('\\t')\n",
    "                self.train_loss = json.loads(line[0])\n",
    "                self.validation_loss = json.loads(line[1])\n",
    "        print(self.train_loss, self.validation_loss)\n",
    "\n",
    "    def training_run(self, epochs=20, batch_size=400, out_file_title=''):\n",
    "\n",
    "        n_batches = int(len(self.triples) / batch_size)\n",
    "        valid_batch = int(len(self.valid_triples) / batch_size) + 1\n",
    "        print(\"batch size: \", n_batches, \"valid_batch: \", valid_batch)\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "\n",
    "            start = time.time()\n",
    "            self.loss = 0.0\n",
    "            self.valid_loss = 0.0\n",
    "            # Normalise the embedding of the entities to 1\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "\n",
    "                batch_samples = random.sample(self.triples, batch_size)\n",
    "\n",
    "                current = []\n",
    "                corrupted = []\n",
    "                for sample in batch_samples:\n",
    "                    corrupted_sample = copy.deepcopy(sample)\n",
    "                    pr = np.random.random(1)[0]\n",
    "                    p = relation_tph[int(corrupted_sample[1])] / (\n",
    "                            relation_tph[int(corrupted_sample[1])] + relation_hpt[int(corrupted_sample[1])])\n",
    "                    if pr < p:\n",
    "                        # change the head entity\n",
    "                        corrupted_sample[0] = random.sample(self.entities, 1)[0] # random.sample(self.entities）从entities中随机选择一个\n",
    "                        while corrupted_sample[0] == sample[0]:  # 保证不是正好替换成了自己本身的三元组\n",
    "                            corrupted_sample[0] = random.sample(self.entities, 1)[0]\n",
    "                    else:\n",
    "                        # change the tail entity\n",
    "                        corrupted_sample[2] = random.sample(self.entities, 1)[0]\n",
    "                        while corrupted_sample[2] == sample[2]: # 保证不是正好替换成了自己本身的三元组\n",
    "                            corrupted_sample[2] = random.sample(self.entities, 1)[0]\n",
    "\n",
    "                    current.append(sample)\n",
    "                    corrupted.append(corrupted_sample)\n",
    "\n",
    "                current = torch.from_numpy(np.array(current)).long() # torch.nn.embedding类的forward只接受longTensor类型的张量\n",
    "                corrupted = torch.from_numpy(np.array(corrupted)).long() # torch.nn.embedding类的forward只接受longTensor类型的张量\n",
    "                self.update_triple_embedding(current, corrupted)\n",
    "\n",
    "            for batch in range(valid_batch):\n",
    "\n",
    "                batch_samples = random.sample(self.valid_triples, batch_size)\n",
    "\n",
    "                current = []\n",
    "                corrupted = []\n",
    "                for sample in batch_samples:\n",
    "                    corrupted_sample = copy.deepcopy(sample)\n",
    "                    pr = np.random.random(1)[0]\n",
    "                    p = relation_tph[int(corrupted_sample[1])] / (\n",
    "                            relation_tph[int(corrupted_sample[1])] + relation_hpt[int(corrupted_sample[1])])\n",
    "\n",
    "                    if pr > p:\n",
    "                        # change the head entity\n",
    "                        corrupted_sample[0] = random.sample(self.entities, 1)[0]\n",
    "                        while corrupted_sample[0] == sample[0]:\n",
    "                            corrupted_sample[0] = random.sample(self.entities, 1)[0]\n",
    "                    else:\n",
    "                        # change the tail entity\n",
    "                        corrupted_sample[2] = random.sample(self.entities, 1)[0]\n",
    "                        while corrupted_sample[2] == sample[2]:\n",
    "                            corrupted_sample[2] = random.sample(self.entities, 1)[0]\n",
    "\n",
    "                    current.append(sample)\n",
    "                    corrupted.append(corrupted_sample)\n",
    "\n",
    "                current = torch.from_numpy(np.array(current)).long()\n",
    "                corrupted = torch.from_numpy(np.array(corrupted)).long()\n",
    "                self.calculate_valid_loss(current, corrupted)\n",
    "\n",
    "            end = time.time()\n",
    "            mean_train_loss = self.loss / n_batches\n",
    "            mean_valid_loss = self.valid_loss / valid_batch\n",
    "            if epoch==epochs-1:\n",
    "                print(\"epoch: \", epoch, \"cost time: %s\" % (round((end - start), 3)))\n",
    "                print(\"Train loss: \", mean_train_loss, \"Valid loss: \", mean_valid_loss)\n",
    "\n",
    "            self.train_loss.append(float(mean_train_loss))\n",
    "            self.validation_loss.append(float(mean_valid_loss))\n",
    "\n",
    "        # visualize the loss as the network trained\n",
    "        # fig = plt.figure(figsize=(6, 4))\n",
    "        # plt.plot(range(1, len(self.train_loss) + 1), self.train_loss, label='Train Loss')\n",
    "        # plt.plot(range(1, len(self.validation_loss) + 1), self.validation_loss, label='Validation Loss')\n",
    "\n",
    "        # plt.xlabel('epochs')\n",
    "        # plt.ylabel('loss')\n",
    "        # plt.xlim(0, len(self.train_loss) + 1)  # consistent scale\n",
    "        # plt.grid(True)\n",
    "        # plt.legend()\n",
    "        # plt.tight_layout()\n",
    "        # plt.title(out_file_title + \"TransE Training loss\")\n",
    "        # plt.show()\n",
    "\n",
    "        # fig.savefig(out_file_title + 'TransE_loss_plot.png', bbox_inches='tight')\n",
    "\n",
    "        with codecs.open(out_file_title + \"TransE_entity_\" + str(self.dimension) + \"dim_batch\" + str(batch_size),\n",
    "                         \"w\") as f1:\n",
    "\n",
    "            for i, e in enumerate(self.model.ent_embedding.weight):\n",
    "                f1.write(str(i) + \"\\t\")\n",
    "                f1.write(str(e.cpu().detach().numpy().tolist()))  # tolist()将矩阵转化成列表\n",
    "                f1.write(\"\\n\")\n",
    "\n",
    "        with codecs.open(out_file_title + \"TransE_relation_\" + str(self.dimension) + \"dim_batch\" + str(batch_size),\n",
    "                         \"w\") as f2:\n",
    "\n",
    "            for i, e in enumerate(self.model.rel_embedding.weight):\n",
    "                f2.write(str(i) + \"\\t\")\n",
    "                f2.write(str(e.cpu().detach().numpy().tolist()))\n",
    "                f2.write(\"\\n\")\n",
    "\n",
    "        with codecs.open(out_file_title + \"loss_record.txt\", \"w\") as f1:\n",
    "            f1.write(str(self.train_loss) + \"\\t\" + str(self.validation_loss))\n",
    "\n",
    "    def update_triple_embedding(self, correct_sample, corrupted_sample):\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.model(correct_sample, corrupted_sample)\n",
    "        self.loss += loss\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "    def calculate_valid_loss(self, correct_sample, corrupted_sample):\n",
    "        loss = self.model(correct_sample, corrupted_sample)\n",
    "        self.valid_loss += loss\n",
    "\n",
    "    def test_run(self, filter=False):\n",
    "\n",
    "        self.filter = filter\n",
    "        hits = 0\n",
    "        rank_sum = 0\n",
    "        num = 0\n",
    "\n",
    "        for triple in tqdm(self.test_triples):\n",
    "            start = time.time()\n",
    "            num += 1\n",
    "            # print(num, triple)\n",
    "            rank_head_dict = {}\n",
    "            rank_tail_dict = {}\n",
    "            #\n",
    "            head_embedding = []\n",
    "            tail_embedding = []\n",
    "            norm_relation = []\n",
    "            hyper_relation = []\n",
    "            tamp = []\n",
    "\n",
    "            head_filter = []\n",
    "            tail_filter = []\n",
    "            if self.filter:\n",
    "\n",
    "                for tr in self.triples:\n",
    "                    if tr[1] == triple[1] and tr[2] == triple[2] and tr[0] != triple[0]:\n",
    "                        head_filter.append(tr)\n",
    "                    if tr[0] == triple[0] and tr[1] == triple[1] and tr[2] != triple[2]:\n",
    "                        tail_filter.append(tr)\n",
    "                for tr in self.test_triples:\n",
    "                    if tr[1] == triple[1] and tr[2] == triple[2] and tr[0] != triple[0]:\n",
    "                        head_filter.append(tr)\n",
    "                    if tr[0] == triple[0] and tr[1] == triple[1] and tr[2] != triple[2]:\n",
    "                        tail_filter.append(tr)\n",
    "                for tr in self.valid_triples:\n",
    "                    if tr[1] == triple[1] and tr[2] == triple[2] and tr[0] != triple[0]:\n",
    "                        head_filter.append(tr)\n",
    "                    if tr[0] == triple[0] and tr[1] == triple[1] and tr[2] != triple[2]:\n",
    "                        tail_filter.append(tr)\n",
    "\n",
    "            for i, entity in enumerate(self.entities): # 遍历所有实体，计算距离\n",
    "\n",
    "                head_triple = [entity, triple[1], triple[2]]\n",
    "                if self.filter:\n",
    "                    if head_triple in head_filter:\n",
    "                        continue\n",
    "                head_embedding.append(head_triple[0])\n",
    "                norm_relation.append(head_triple[1])\n",
    "                tail_embedding.append(head_triple[2])\n",
    "\n",
    "                tamp.append(tuple(head_triple))\n",
    "\n",
    "            head_embedding = torch.from_numpy(np.array(head_embedding)).long()\n",
    "            norm_relation = torch.from_numpy(np.array(norm_relation)).long()\n",
    "            tail_embedding = torch.from_numpy(np.array(tail_embedding)).long()\n",
    "            distance = self.model.test_distance(head_embedding, norm_relation, tail_embedding)\n",
    "\n",
    "            for i in range(len(tamp)):\n",
    "                rank_head_dict[tamp[i]] = distance[i]\n",
    "\n",
    "            head_embedding = []\n",
    "            tail_embedding = []\n",
    "            norm_relation = []\n",
    "            hyper_relation = []\n",
    "            tamp = []\n",
    "\n",
    "            for i, tail in enumerate(self.entities):\n",
    "\n",
    "                tail_triple = [triple[0], triple[1], tail]\n",
    "                if self.filter:\n",
    "                    if tail_triple in tail_filter:\n",
    "                        continue\n",
    "                head_embedding.append(tail_triple[0])\n",
    "                norm_relation.append(tail_triple[1])\n",
    "                tail_embedding.append(tail_triple[2])\n",
    "                tamp.append(tuple(tail_triple))\n",
    "\n",
    "            head_embedding = torch.from_numpy(np.array(head_embedding)).long()\n",
    "            norm_relation = torch.from_numpy(np.array(norm_relation)).long()\n",
    "            tail_embedding = torch.from_numpy(np.array(tail_embedding)).long()\n",
    "\n",
    "            distance = self.model.test_distance(head_embedding, norm_relation, tail_embedding)\n",
    "            for i in range(len(tamp)):\n",
    "                rank_tail_dict[tamp[i]] = distance[i]\n",
    "\n",
    "            rank_head_sorted = sorted(rank_head_dict.items(), key=operator.itemgetter(1), reverse=False)\n",
    "            rank_tail_sorted = sorted(rank_tail_dict.items(), key=operator.itemgetter(1), reverse=False)\n",
    "\n",
    "            # calculate the mean_rank and hit_10\n",
    "            # head data\n",
    "            i = 0\n",
    "            for i in range(len(rank_head_sorted)):\n",
    "                if triple[0] == rank_head_sorted[i][0][0]:\n",
    "                    if i < 10:\n",
    "                        hits += 1\n",
    "                    rank_sum = rank_sum + i + 1\n",
    "                    break\n",
    "\n",
    "            # tail rank\n",
    "            i = 0\n",
    "            for i in range(len(rank_tail_sorted)):\n",
    "                if triple[2] == rank_tail_sorted[i][0][2]:\n",
    "                    if i < 10:\n",
    "                        hits += 1\n",
    "                    rank_sum = rank_sum + i + 1\n",
    "                    break\n",
    "            end = time.time()\n",
    "            # print(\"epoch: \", num, \"cost time: %s\" % (round((end - start), 3)), str(hits / (2 * num)),\n",
    "            #       str(rank_sum / (2 * num)))\n",
    "        self.hit_10 = hits / (2 * len(self.test_triples))\n",
    "        self.mean_rank = rank_sum / (2 * len(self.test_triples))\n",
    "        print(\"hits@10: \", self.hit_10)\n",
    "        print(\"meanrank: \", self.mean_rank)\n",
    "        f = open(\"./result.txt\", 'w')\n",
    "        f.write(\"hits@10: \" + str(self.hit_10) + '\\n')\n",
    "        f.write(\"meanrank: \" + str(self.mean_rank) + '\\n')\n",
    "        f.close()\n",
    "        return self.hit_10, self.mean_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load file...\n",
      "Complete load. entity : 14951 , relation : 1345 , train triple : 483142, , valid triple : 50000\n",
      "margin:  1\n",
      "batch size:  9 valid_batch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [53:48<00:00,  6.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  499 cost time: 6.824\n",
      "Train loss:  tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>) Valid loss:  tensor(0.0862, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 59071/59071 [45:40<00:00, 21.56it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@10:  0.4240067037971255\n",
      "meanrank:  288.3658309491967\n",
      "margin:  2\n",
      "batch size:  9 valid_batch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [53:57<00:00,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  499 cost time: 6.86\n",
      "Train loss:  tensor(0.0170, device='cuda:0', grad_fn=<DivBackward0>) Valid loss:  tensor(0.2059, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 59071/59071 [44:46<00:00, 21.99it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@10:  0.45693318210289313\n",
      "meanrank:  247.69046571075486\n",
      "margin:  4\n",
      "batch size:  9 valid_batch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [56:11<00:00,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  499 cost time: 6.717\n",
      "Train loss:  tensor(0.1156, device='cuda:0', grad_fn=<DivBackward0>) Valid loss:  tensor(0.6143, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 59071/59071 [46:14<00:00, 21.29it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@10:  0.40451321291327386\n",
      "meanrank:  265.2810600802424\n",
      "margin:  5\n",
      "batch size:  9 valid_batch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [56:14<00:00,  6.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  499 cost time: 6.647\n",
      "Train loss:  tensor(0.2324, device='cuda:0', grad_fn=<DivBackward0>) Valid loss:  tensor(0.9034, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 59071/59071 [46:43<00:00, 21.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@10:  0.3649675813851128\n",
      "meanrank:  316.00402058539726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file1 = \"FB15k/freebase_mtr100_mte100-train.txt\"  # 就是把train.txt的尾实体和关系换个了位置，保证头实体、关系、尾实体的位置\n",
    "file2 = \"FB15k/entity2id.txt\"\n",
    "file3 = \"FB15k/relation2id.txt\"\n",
    "file4 = \"FB15k/freebase_mtr100_mte100-valid.txt\"\n",
    "\n",
    "file5 = \"KB15k_torch_TransE_entity_50dim_batch50000\"\n",
    "file6 = \"KB15k_torch_TransE_relation_50dim_batch50000\"\n",
    "\n",
    "file8 = \"FB15k/freebase_mtr100_mte100-test.txt\"\n",
    "# file9 = \"Fb15k_loss_record.txt\"\n",
    "entity_set, relation_set, triple_list, valid_triple_list = dataloader(file1, file2, file3, file4)\n",
    "margins=[1,2,4,5]\n",
    "for margin in margins:\n",
    "    transE = TransE(entity_set, relation_set, triple_list, embedding_dim=50, lr=0.001, margin=margin, norm=1, C = 0.25, valid_triple_list=valid_triple_list)\n",
    "    transE.data_initialise()\n",
    "    print(\"margin: \",margin)\n",
    "    transE.training_run(epochs=500, batch_size=50000, out_file_title=\"KB15k_torch_\")\n",
    "    transE.insert_test_data(file5, file6, file8)\n",
    "    transE.test_run(filter=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transE.insert_test_data(file5, file6, file8)\n",
    "\n",
    "transE.test_run(filter=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
