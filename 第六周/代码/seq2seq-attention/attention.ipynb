{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "import jieba\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')      # 每一行是英文+翻译的形式\n",
    "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
    "            cn.append(['BOS'] +  list(jieba.cut(line[1]))+ ['EOS'])\n",
    "    return en, cn\n",
    "\n",
    "train_file = 'nmt/train.txt'\n",
    "dev_file = 'nmt/dev.txt'\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014\n",
      "3031\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter() \n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1  \n",
    "    ls = word_count.most_common(max_words) \n",
    "    print(len(ls)) \n",
    "    total_words = len(ls) + 2\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    \n",
    "    word_dict[\"UNK\"] = UNK_IDX \n",
    "    word_dict[\"PAD\"] = PAD_IDX \n",
    "    return word_dict, total_words\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en) \n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(n, batch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, batch_size) \n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    batches = []\n",
    "    for idx in idx_list:\n",
    "        batches.append(np.arange(idx, min(idx + batch_size, n)))\n",
    "    return batches\n",
    "\n",
    "def sent_padding(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs) \n",
    "    max_len = np.max(lengths) # 取出最长的的语句长度\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths \n",
    "\n",
    "def get_examples(en_sentences, cn_sentences, batch_size):\n",
    "    batches = get_batches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for batch in batches: \n",
    "        mb_en_sentences = [en_sentences[t] for t in batch]        \n",
    "        mb_cn_sentences = [cn_sentences[t] for t in batch]\n",
    "        # padding\n",
    "        mb_x, mb_x_len = sent_padding(mb_en_sentences)\n",
    "        mb_y, mb_y_len = sent_padding(mb_cn_sentences)\n",
    "        \n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "        # （英文句子，英文句子长度，中文句子，中文句子长度） \n",
    "    return all_ex\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_data = get_examples(train_en, train_cn, batch_size)  # (mb_x, mb_x_len, mb_y, mb_y_len)\n",
    "random.shuffle(train_data) \n",
    "dev_data = get_examples(dev_en, dev_cn, batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型架构\n",
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        # hid: [2, batch_size, enc_hidden_size]\n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1) # 将最后一层的hid的双向拼接\n",
    "        # hid: [batch_size, 2*enc_hidden_size]\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "        # hid: [1, batch_size, dec_hidden_size]\n",
    "        # out: [batch_size, seq_len, 2*enc_hidden_size]\n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        # enc_hidden_size跟Encoder的一样\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1) # input_len = context_len\n",
    "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                \n",
    "            batch_size, input_len, -1) # batch_size, context_len, dec_hidden_size\n",
    "        attn = torch.bmm(output, context_in.transpose(1,2)) \n",
    "        attn = F.softmax(attn, dim=2) \n",
    "        context = torch.bmm(attn, context) \n",
    "        output = torch.cat((context, output), dim=2) \n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output)) \n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2,hidden_size=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=device)[None, :] < y_len[:, None]\n",
    "        mask = ( ~ x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, encoder_out, x_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        mask = self.create_mask(y_lengths, x_lengths)\n",
    "        output, attn = self.attention(output_seq, encoder_out, mask) \n",
    "        output = F.log_softmax(self.out(output), -1) \n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        # print(hid.shape)=torch.Size([1, batch_size, dec_hidden_size])\n",
    "        # print(out.shape)=torch.Size([batch_size, seq_len, 2*enc_hidden_size])\n",
    "        output, hid, attn = self.decoder(encoder_out=encoder_out, \n",
    "                    x_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        # output =(batch_size, output_len, vocab_size)\n",
    "        # hid.shape = (1, batch_size, dec_hidden_size)\n",
    "        # attn.shape = (batch_size, output_len, context_len)\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(encoder_out=encoder_out, \n",
    "                    x_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "## 参数选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words,\n",
    "                    embed_size=embed_size,\n",
    "                    enc_hidden_size=hidden_size,\n",
    "                    dec_hidden_size=hidden_size,\n",
    "                    dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words,\n",
    "                    embed_size=embed_size,\n",
    "                    enc_hidden_size=hidden_size,\n",
    "                    dec_hidden_size=hidden_size,\n",
    "                    dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            #（英文batch，英文长度，中文batch，中文长度）         \n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()  \n",
    "            # 前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "           \n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            \n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()  # 下三角矩阵\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()  # 一个batch里多少个单词 \n",
    "            total_loss += loss.item() * num_words \n",
    "            total_num_words += num_words\n",
    "          \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            #为了防止梯度过大，设置梯度的阈值 \n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():#不需要更新模型，不需要梯度\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)   \n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    # 这里传入bos作为首个单词的输入\n",
    "    #translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]], device='cuda:0')\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "     \n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\": \n",
    "            trans.append(word) \n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.011500358581543\n",
      "Epoch 0 Training loss 6.756094622533424\n",
      "Evaluation loss 5.6999799464571765\n",
      "Epoch 1 iteration 0 loss 5.798335075378418\n",
      "Epoch 1 Training loss 5.466534294509326\n",
      "Epoch 2 iteration 0 loss 5.529472351074219\n",
      "Epoch 2 Training loss 5.285823825766366\n",
      "Epoch 3 iteration 0 loss 5.407910346984863\n",
      "Epoch 3 Training loss 5.166656795135513\n",
      "Epoch 4 iteration 0 loss 5.3240885734558105\n",
      "Epoch 4 Training loss 5.073005681552489\n",
      "Epoch 5 iteration 0 loss 5.256463050842285\n",
      "Epoch 5 Training loss 4.991418913231434\n",
      "Evaluation loss 5.405315972189129\n",
      "Epoch 6 iteration 0 loss 5.183313369750977\n",
      "Epoch 6 Training loss 4.9111994279650535\n",
      "Epoch 7 iteration 0 loss 5.100142478942871\n",
      "Epoch 7 Training loss 4.8293622338364735\n",
      "Epoch 8 iteration 0 loss 5.013706684112549\n",
      "Epoch 8 Training loss 4.751505716461744\n",
      "Epoch 9 iteration 0 loss 4.955442905426025\n",
      "Epoch 9 Training loss 4.669564300295297\n",
      "Epoch 10 iteration 0 loss 4.870354175567627\n",
      "Epoch 10 Training loss 4.586660949672792\n",
      "Evaluation loss 5.209082232446269\n",
      "Epoch 11 iteration 0 loss 4.777505874633789\n",
      "Epoch 11 Training loss 4.5003964540358385\n",
      "Epoch 12 iteration 0 loss 4.68989372253418\n",
      "Epoch 12 Training loss 4.413808827948231\n",
      "Epoch 13 iteration 0 loss 4.588587284088135\n",
      "Epoch 13 Training loss 4.330431551813252\n",
      "Epoch 14 iteration 0 loss 4.506185054779053\n",
      "Epoch 14 Training loss 4.247406869823706\n",
      "Epoch 15 iteration 0 loss 4.41749382019043\n",
      "Epoch 15 Training loss 4.165014995999327\n",
      "Evaluation loss 5.0606111688412385\n",
      "Epoch 16 iteration 0 loss 4.324779987335205\n",
      "Epoch 16 Training loss 4.078872099755966\n",
      "Epoch 17 iteration 0 loss 4.24416446685791\n",
      "Epoch 17 Training loss 3.9979512981726155\n",
      "Epoch 18 iteration 0 loss 4.158228874206543\n",
      "Epoch 18 Training loss 3.9279776625442318\n",
      "Epoch 19 iteration 0 loss 4.057202339172363\n",
      "Epoch 19 Training loss 3.841875289367698\n",
      "Epoch 20 iteration 0 loss 3.9854986667633057\n",
      "Epoch 20 Training loss 3.764947683294093\n",
      "Evaluation loss 5.090508030608874\n",
      "Epoch 21 iteration 0 loss 3.8934712409973145\n",
      "Epoch 21 Training loss 3.6911155325560103\n",
      "Epoch 22 iteration 0 loss 3.817307233810425\n",
      "Epoch 22 Training loss 3.62145330643271\n",
      "Epoch 23 iteration 0 loss 3.7760233879089355\n",
      "Epoch 23 Training loss 3.551729200498376\n",
      "Epoch 24 iteration 0 loss 3.7313332557678223\n",
      "Epoch 24 Training loss 3.4928980078957004\n",
      "Epoch 25 iteration 0 loss 3.610400915145874\n",
      "Epoch 25 Training loss 3.4276013128737914\n",
      "Evaluation loss 5.081439504304711\n",
      "Epoch 26 iteration 0 loss 3.54899001121521\n",
      "Epoch 26 Training loss 3.3525285212089666\n",
      "Epoch 27 iteration 0 loss 3.4885635375976562\n",
      "Epoch 27 Training loss 3.259987780676844\n",
      "Epoch 28 iteration 0 loss 3.3792455196380615\n",
      "Epoch 28 Training loss 3.1690000485808825\n",
      "Epoch 29 iteration 0 loss 3.304288864135742\n",
      "Epoch 29 Training loss 3.0930701264759306\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you may go anywhere . EOS\n",
      "BOS 你 可以 随便 去 哪儿 。 EOS\n",
      "你可以任何你。\n",
      "\n",
      "BOS do n't UNK me . EOS\n",
      "BOS UNK UNK 我 。 EOS\n",
      "請我。\n",
      "\n",
      "BOS here 's your tea . EOS\n",
      "BOS 这 是 你 的 UNK 。 EOS\n",
      "你是你的。\n",
      "\n",
      "BOS i 'm UNK UNK . EOS\n",
      "BOS 我 真是 UNK 。 EOS\n",
      "我的房子。\n",
      "\n",
      "BOS tom could be UNK . EOS\n",
      "BOS 汤姆 可能 是 UNK 。 EOS\n",
      "汤姆是个好。\n",
      "\n",
      "BOS what happened that night ? EOS\n",
      "BOS 这个 晚上 发生 了 什么 ？ EOS\n",
      "這是什麼？\n",
      "\n",
      "BOS prices are going up . EOS\n",
      "BOS UNK 了 。 EOS\n",
      "这是个一个。\n",
      "\n",
      "BOS i took a shower . EOS\n",
      "BOS 我 洗 了 澡 。 EOS\n",
      "我已經了。\n",
      "\n",
      "BOS how is your wife ? EOS\n",
      "BOS 你 太太 怎么样 ？ EOS\n",
      "我可以了什麼？\n",
      "\n",
      "BOS please UNK your name . EOS\n",
      "BOS 请 UNK 一下 您 的 名字 。 EOS\n",
      "你想你的東西。\n",
      "\n",
      "BOS have you eaten lunch ? EOS\n",
      "BOS 你 吃 UNK 飯 了 嗎 ？ EOS\n",
      "你在哪里？\n",
      "\n",
      "BOS it 's our pleasure . EOS\n",
      "BOS 这是 我们 的 UNK 。 EOS\n",
      "这是个好。\n",
      "\n",
      "BOS i ca n't remember . EOS\n",
      "BOS 我 UNK 來 。 EOS\n",
      "我不喜欢。\n",
      "\n",
      "BOS he traveled on business . EOS\n",
      "BOS 他 旅行 UNK 。 EOS\n",
      "他是个好了。\n",
      "\n",
      "BOS is his UNK regular ? EOS\n",
      "BOS 他 的 UNK UNK UNK 嗎 ？ EOS\n",
      "他是什麼？\n",
      "\n",
      "BOS he 's very UNK . EOS\n",
      "BOS 他 说话 很 直接 。 EOS\n",
      "他是个好。\n",
      "\n",
      "BOS my UNK are UNK . EOS\n",
      "BOS 我 的 UNK UNK 。 EOS\n",
      "我的房子的房子。\n",
      "\n",
      "BOS she works very hard . EOS\n",
      "BOS 她 很 努力 工作 。 EOS\n",
      "她在這裡了。\n",
      "\n",
      "BOS i have to win . EOS\n",
      "BOS 我 必须 赢 。 EOS\n",
      "我想了。\n",
      "\n",
      "BOS this turkey tastes good . EOS\n",
      "BOS 这 只 UNK 味道 很 好 。 EOS\n",
      "别没有人。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmcot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
