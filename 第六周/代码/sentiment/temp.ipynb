{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED) #为CPU设置随机种子\n",
    "torch.cuda.manual_seed(SEED)#为GPU设置随机种子\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy',tokenizer_language='en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED)) #默认split_ratio=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:20<00:00, 19617.64it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "                                                        (train_data, valid_data, test_data), \n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WordAVGModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim) #output_dim=1\n",
    "        \n",
    "    def forward(self, text): # text: [seq_len,batch_size]\n",
    "        embedded = self.embedding(text)  # embedded = [seq_len, batch_size, embedding_dim] \n",
    "        embedded = embedded.permute(1, 0, 2) # [batch_size, seq_len, embedding_dim]  \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)   \n",
    "        return self.fc(pooled)  # batch_size, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SZIE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1 \n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] \n",
    "\n",
    "model = WordAVGModel(VOCAB_SZIE, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,500,301 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.3896, -0.0554,  0.4922,  ..., -0.0182, -0.8245,  0.0696],\n",
       "        [ 0.2828,  0.4366,  0.7027,  ..., -1.0296, -0.0782,  0.6539],\n",
       "        [ 2.4727, -0.4058,  0.2144,  ...,  0.5650,  1.4462, -0.9035]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] # UNK_IDX=0\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM) \n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters()) \n",
    "loss_fn = nn.BCEWithLogitsLoss()  #BCEWithLogitsLoss：二分类损失函数\n",
    "model = model.to(device) \n",
    "loss_fn = loss_fn.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y): \n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "    model.train() \n",
    "    for batch in iterator: \n",
    "        optimizer.zero_grad() \n",
    "        preds = model(batch.text).squeeze(1)\n",
    "        loss = loss_fn(preds, batch.label)\n",
    "        acc = binary_accuracy(preds, batch.label)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)   \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_len = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "        for batch in iterator: \n",
    "            \n",
    "            preds = model(batch.text).squeeze(1)\n",
    "            loss = loss_fn(preds, batch.label)\n",
    "            acc = binary_accuracy(preds, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item() * len(batch.label)\n",
    "            epoch_acc += acc.item() * len(batch.label)\n",
    "            total_len += len(batch.label)\n",
    "    model.train()\n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def epoch_time(start_time, end_time): \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:05<01:49,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.686 | Train Acc: 59.51%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 69.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:11<01:44,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.642 | Train Acc: 75.42%\n",
      "\t Val. Loss: 0.511 |  Val. Acc: 76.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:17<01:38,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.566 | Train Acc: 79.75%\n",
      "\t Val. Loss: 0.452 |  Val. Acc: 80.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:22<01:31,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.493 | Train Acc: 83.65%\n",
      "\t Val. Loss: 0.430 |  Val. Acc: 83.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:28<01:24,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.431 | Train Acc: 86.48%\n",
      "\t Val. Loss: 0.438 |  Val. Acc: 84.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:33<01:14,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.380 | Train Acc: 88.06%\n",
      "\t Val. Loss: 0.447 |  Val. Acc: 86.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:38<01:07,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.341 | Train Acc: 89.42%\n",
      "\t Val. Loss: 0.474 |  Val. Acc: 86.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:43<01:02,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.311 | Train Acc: 90.34%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 87.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:48<00:58,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.286 | Train Acc: 91.07%\n",
      "\t Val. Loss: 0.521 |  Val. Acc: 87.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:54<00:53,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.266 | Train Acc: 91.85%\n",
      "\t Val. Loss: 0.543 |  Val. Acc: 87.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:59<00:47,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.245 | Train Acc: 92.31%\n",
      "\t Val. Loss: 0.566 |  Val. Acc: 88.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:04<00:41,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.229 | Train Acc: 92.93%\n",
      "\t Val. Loss: 0.587 |  Val. Acc: 88.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:09<00:35,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.215 | Train Acc: 93.30%\n",
      "\t Val. Loss: 0.606 |  Val. Acc: 88.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:14<00:30,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.202 | Train Acc: 93.79%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 88.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:19<00:25,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.192 | Train Acc: 94.13%\n",
      "\t Val. Loss: 0.645 |  Val. Acc: 88.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [01:24<00:20,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.179 | Train Acc: 94.53%\n",
      "\t Val. Loss: 0.663 |  Val. Acc: 88.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [01:29<00:15,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.169 | Train Acc: 94.91%\n",
      "\t Val. Loss: 0.683 |  Val. Acc: 88.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [01:34<00:10,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.159 | Train Acc: 95.15%\n",
      "\t Val. Loss: 0.702 |  Val. Acc: 88.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [01:39<00:05,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.151 | Train Acc: 95.45%\n",
      "\t Val. Loss: 0.723 |  Val. Acc: 88.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:44<00:00,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.142 | Train Acc: 95.83%\n",
      "\t Val. Loss: 0.741 |  Val. Acc: 89.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf') \n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn) # 得到训练集每个epoch的平均损失和准确率\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn) # 验证集每个epoch的平均损失和准确率，model传入的是训练完的参数\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss: \n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'wordavg-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"wordavg-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.431 | Test Acc: 82.54%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('wordavg-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  \n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "def predict_sentiment(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)] \n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized] \n",
    "    tensor = torch.LongTensor(indexed).to(device) #seq_len\n",
    "    tensor = tensor.unsqueeze(1)    \n",
    "    pred = torch.sigmoid(model(tensor))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001455277670174837"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text): # text=[seq_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(text)) #[seq_len, batch_size, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))    \n",
    "        return self.fc(hidden.squeeze(0)) # #sequeeze()只有维度为1才会去掉，输出[batch size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.3896, -0.0554,  0.4922,  ..., -0.0182, -0.8245,  0.0696],\n",
      "        [ 0.2828,  0.4366,  0.7027,  ..., -1.0296, -0.0782,  0.6539],\n",
      "        [ 2.4727, -0.4058,  0.2144,  ...,  0.5650,  1.4462, -0.9035]])\n"
     ]
    }
   ],
   "source": [
    "# 同上初始化\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:46<14:43, 46.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.679 | Train Acc: 56.73%\n",
      "\t Val. Loss: 0.633 |  Val. Acc: 63.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:32<13:50, 46.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.673 | Train Acc: 56.59%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 61.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:19<13:08, 46.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.594 | Train Acc: 68.03%\n",
      "\t Val. Loss: 0.565 |  Val. Acc: 73.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [03:06<12:31, 47.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.478 | Train Acc: 78.77%\n",
      "\t Val. Loss: 0.426 |  Val. Acc: 82.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [03:53<11:42, 46.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.340 | Train Acc: 86.37%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 87.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [04:40<10:57, 46.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.277 | Train Acc: 89.14%\n",
      "\t Val. Loss: 0.297 |  Val. Acc: 88.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [05:29<10:16, 47.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.243 | Train Acc: 90.57%\n",
      "\t Val. Loss: 0.292 |  Val. Acc: 88.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [06:16<09:30, 47.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.212 | Train Acc: 92.07%\n",
      "\t Val. Loss: 0.368 |  Val. Acc: 87.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [07:03<08:40, 47.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.190 | Train Acc: 92.87%\n",
      "\t Val. Loss: 0.280 |  Val. Acc: 89.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [07:51<07:54, 47.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.163 | Train Acc: 93.93%\n",
      "\t Val. Loss: 0.301 |  Val. Acc: 89.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [08:37<07:03, 47.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.145 | Train Acc: 94.74%\n",
      "\t Val. Loss: 0.295 |  Val. Acc: 89.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [09:24<06:15, 46.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.128 | Train Acc: 95.21%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 89.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [10:11<05:28, 46.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.115 | Train Acc: 95.66%\n",
      "\t Val. Loss: 0.328 |  Val. Acc: 89.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [10:57<04:41, 46.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.103 | Train Acc: 96.45%\n",
      "\t Val. Loss: 0.364 |  Val. Acc: 89.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [11:44<03:54, 46.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.087 | Train Acc: 96.97%\n",
      "\t Val. Loss: 0.373 |  Val. Acc: 89.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [12:31<03:07, 46.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.082 | Train Acc: 97.07%\n",
      "\t Val. Loss: 0.365 |  Val. Acc: 89.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [13:20<02:22, 47.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.074 | Train Acc: 97.40%\n",
      "\t Val. Loss: 0.437 |  Val. Acc: 88.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [14:07<01:34, 47.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.063 | Train Acc: 97.77%\n",
      "\t Val. Loss: 0.389 |  Val. Acc: 89.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [14:53<00:47, 47.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.058 | Train Acc: 98.14%\n",
      "\t Val. Loss: 0.405 |  Val. Acc: 89.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [15:41<00:00, 47.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.052 | Train Acc: 98.22%\n",
      "\t Val. Loss: 0.412 |  Val. Acc: 89.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 用kaggleGPU跑花了40分钟。\n",
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, loss_fn)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, loss_fn)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.305 | Test Acc: 87.90%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('lstm-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, loss_fn)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02666657418012619"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"I feel This film bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967237114906311"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1, out_channels = n_filters, \n",
    "                                    kernel_size = (fs, embedding_dim)) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0) # [batch size, sent len]\n",
    "        embedded = self.embedding(text) # [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1) # [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]      \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "  \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,620,801 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN(VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model = model.to(device)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "# 比averge model模型参数差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.649 | Train Acc: 61.61%\n",
      "\t Val. Loss: 0.507 |  Val. Acc: 78.01%\n",
      "Epoch: 02 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.423 | Train Acc: 80.55%\n",
      "\t Val. Loss: 0.367 |  Val. Acc: 83.43%\n",
      "Epoch: 03 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.304 | Train Acc: 87.35%\n",
      "\t Val. Loss: 0.330 |  Val. Acc: 85.68%\n",
      "Epoch: 04 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.26%\n",
      "\t Val. Loss: 0.323 |  Val. Acc: 86.89%\n",
      "Epoch: 05 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.162 | Train Acc: 93.90%\n",
      "\t Val. Loss: 0.335 |  Val. Acc: 86.88%\n",
      "Epoch: 06 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.109 | Train Acc: 96.12%\n",
      "\t Val. Loss: 0.352 |  Val. Acc: 86.89%\n",
      "Epoch: 07 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.080 | Train Acc: 97.23%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 87.05%\n",
      "Epoch: 08 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.056 | Train Acc: 98.25%\n",
      "\t Val. Loss: 0.400 |  Val. Acc: 87.48%\n",
      "Epoch: 09 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.71%\n",
      "\t Val. Loss: 0.425 |  Val. Acc: 86.93%\n",
      "Epoch: 10 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.033 | Train Acc: 99.03%\n",
      "\t Val. Loss: 0.455 |  Val. Acc: 87.04%\n",
      "Epoch: 11 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.29%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 86.81%\n",
      "Epoch: 12 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.41%\n",
      "\t Val. Loss: 0.517 |  Val. Acc: 86.88%\n",
      "Epoch: 13 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.019 | Train Acc: 99.47%\n",
      "\t Val. Loss: 0.541 |  Val. Acc: 87.17%\n",
      "Epoch: 14 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.66%\n",
      "\t Val. Loss: 0.556 |  Val. Acc: 86.92%\n",
      "Epoch: 15 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.012 | Train Acc: 99.71%\n",
      "\t Val. Loss: 0.596 |  Val. Acc: 87.40%\n",
      "Epoch: 16 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.013 | Train Acc: 99.66%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 86.91%\n",
      "Epoch: 17 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.010 | Train Acc: 99.74%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 86.60%\n",
      "Epoch: 18 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.011 | Train Acc: 99.70%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 86.76%\n",
      "Epoch: 19 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.009 | Train Acc: 99.75%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 87.04%\n",
      "Epoch: 20 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.007 | Train Acc: 99.77%\n",
      "\t Val. Loss: 0.711 |  Val. Acc: 86.99%\n"
     ]
    }
   ],
   "source": [
    "# 同上，需要花8分钟左右\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'CNN-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.340 | Test Acc: 85.48%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('CNN-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36543038487434387"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"I feel This film bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9817049503326416"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is great well\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmcot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
