# NMT、Seq2seq
## SMT
在过去，翻译系统是基于概率模型构建的：$$=\text{argmax}_yP(x|y)P(y)$$
- **翻译模型** ，告诉我们源语言基于翻译概率最可能被翻译的一些结果候选。
- **语言模型** ，结合语句的通顺度等综合评估最优的翻译结果。
## Seq2seq
Seq2Seq是一个典型的深度学习模型，在更高的层面上，Seq2Seq 是一个有两个RNN组成的端到端模型
- 一个encoder 编码器，将模型的输入序列作为输入，然后编码固定大小的“上下文向量”。
- 一个decoder 解码器，使用来自编码器生成的上下文向量作为从其生成输出序列的“种子”。
因此，Seq2Seq 模型通常被称为“**编码器-解码器模型**”。
![Pasted image 20240803153951](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803153951.png)
损失函数为：![Pasted image 20240803154904](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803154904.png)
在推理过程中，默认采用的是greedy decoding，该方法的缺点是当前的最大概率单词并不一定是最优的，因为decoding的时候只选了一条路径，因此其他的路径无法选择。
改进方法：Beam search decoding，该方法的核心思想是：在decoder的每一步中，跟踪k个最可能的部分翻译。**k一般为5-10**。
判断标准：计算分数$$\text{score}(y_1,...,y_t)=\frac{1}{t}\sum_{i=1}\log P_{LM}(y_i|y_1,...,y_{i-1},x)$$使用归一化$\frac{1}{t}$的原因是，长的翻译句子分数会更低，短句子分数更高，所以在选取时需要做归一化处理。
![Pasted image 20240803155219](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803155219.png)
### BLEU
BLEU（Bilingual Evaluation Understudy）可以衡量模型翻译的质量，它通过计算机器翻译输出和一组参考翻译之间的 n-gram 重合度来评估翻译质量。计算方式：$$\text{BLEU} = BP \cdot \exp \left( \sum_{n=1}^N \frac{1}{N} \log p_n \right)$$其中：$$BP = \begin{cases} 1, & \text{if } c > r \\ \exp\left(1 - \frac{r}{c}\right), & \text{if } c \leq r \end{cases}$$
c是机器翻译输出的总词数，r是参考翻译的总词数。
# QA系统
从文件或段落中找到相关的答案，这一过程也被称作**Reading Comprehension阅读理解**。
## BiDAF
![Pasted image 20240803160332](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803160332.png)
BiDAF（Bi-Directional Attention Flow）是用作阅读理解的模型。
### 模型架构
从下往上：
1. Encoding
	1. 有字符、词各自嵌入层，字符嵌入层提取字符级别的特征然后和词嵌入结合，结果证明添加字符嵌入效果更好。 ![Pasted image 20240803160523](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803160523.png)
	2. 然后使用BiLSTM来从两个方向捕获上下文信息![Pasted image 20240803160912](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803160912.png)
2. Attention
	1. 有两个attention，分别是context-to-query attention和query-to-context attention。![Pasted image 20240803161016](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803161016.png)
3. Modeling和输出层
	1. Modeling将原始的上下文表示与注意力流层生成的表示结合，通过双向LSTM进一步处理，生成更高层次的上下文表示![Pasted image 20240803161101](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803161101.png)
	2. 输出层使用softmax输出结果。
## 结果比对
Bert比BiDAF的结果好的原因：Bert可以认为是包含了BiDAF。
Bert有`attention(P,P)+attention(P,Q)+attention(Q,P)+attention(Q,Q)`，而BiDAF只有中间两个。
![Pasted image 20240803161240](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803161240.png)

# Neural Language Generation
## Formalizing NLG: a simple model and training algorithm
按照文本序列将token逐个输入，模型根据context来预测下一个词在此表上的概率分布。
![Pasted image 20240803161920](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803161920.png)
损失函数：![Pasted image 20240803162049](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803162049.png)
自回归模型使用teacher-forcing方式：对于每个时间步 t，模型的输入是目标序列的实际词汇，而不是模型在前一个时间步生成的预测词汇。这意味着模型在训练时“被强制”使用正确的目标词作为输入。
## Decoding from NLG models
Greedy methods：选择令每个位置或者整个序列概率最大的词，但是这种方法容易得到内容重复的文本。![Pasted image 20240803162411](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803162411.png)

Sampling：在所有词上采样，但是有很多跟上下文无关的词，改进方法：top-k sampling，只在概率最大的前k个token中进行。![Pasted image 20240803162541](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240803162541.png)