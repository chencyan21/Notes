# Mistralç®€ä»‹
Mistralæœ‰å¤šä¸ªæ¨¡å‹ï¼Œå…¶ä¸­çš„**8Ã—7Bæ¨¡å‹**é‡‡ç”¨ä¸“å®¶ï¼ˆFFNï¼‰æ··åˆç­–ç•¥å¯¹æ ‡å‡†çš„transformeræ¶æ„è¿›è¡Œæ”¹é€ ï¼Œ  åœ¨è¯¥æ¨¡å‹ä¸­æœ‰8ä¸ªå‰å‘åé¦ˆç½‘ç»œè¢«ç§°ä¸ºâ€œä¸“å®¶expertsâ€ã€‚åœ¨inferenceçš„æ—¶å€™ï¼Œå¦ä¸€ä¸ªé—¨æ§ç¥ç»ç½‘ç»œï¼ˆrouterï¼‰ä¼šé¦–å…ˆé€‰æ‹©å…¶ä¸­ä¸¤ä¸ªä¸“å®¶æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç„¶åå–å…¶åŠ æƒå¹³å‡å€¼æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªtokenã€‚
è¿™ç§ç­–ç•¥è®©mistralæ—¢æœ‰å¤§å‹æ¨¡å‹çš„æ€§èƒ½æå‡ï¼Œåˆæœ‰å’Œå°å‹æ¨¡å‹ç›¸å½“çš„æ¨ç†æˆæœ¬ã€‚æ¨¡å‹æœ‰400å¤šäº¿ä¸ªå‚æ•°ï¼Œä½†æ˜¯åœ¨æ¨ç†æ—¶åªç”¨åˆ°160+äº¿ ã€‚
Mistralå¦ä¸€ä¸ªå®ç”¨çš„ç‰¹æ€§æ˜¯**è§£æjson**ï¼Œå½“llmé›†æˆåˆ°æ›´å¤§çš„è½¯ä»¶åº”ç”¨ä¸­æ—¶ï¼Œllmèƒ½å¤Ÿè¿”å›ç»“æ„åŒ–çš„jsonæ ¼å¼ã€‚
Mistralæ¨¡å‹æœ‰å¼€æºçš„Mistral 7Bå’ŒMistral 8X7Bï¼Œå•†ç”¨çš„Mistral smallã€Mistral mediumå’ŒMistral largeã€‚
# Overview
æ¨¡å‹åŸºäºä¸€ä¸ªç”±ä¸¤å±‚ç»„æˆçš„transformerå—ï¼Œåˆ†åˆ«æ˜¯ä¸€ä¸ªå‰é¦ˆå±‚å’Œä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ã€‚ä¸ºäº†å¢åŠ æ¨¡å‹å®¹é‡ï¼Œå¤åˆ¶äº†Næ¬¡å‰é¦ˆç½‘ç»œå±‚ã€‚ä½¿ç”¨ä¸€ä¸ªrouterè·¯ç”±å™¨å°†æ¯ä¸ªtokenæ˜ å°„åˆ°é¡¶éƒ¨çš„kä¸ªå‰é¦ˆå±‚ï¼Œå¹¶å¿½ç•¥å…¶ä»–çš„å±‚ã€‚
![Pasted image 20240807200956](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240807200956.png)
å•†ç”¨çš„æ¨¡å‹ä¸­ï¼ŒMistral largeçš„æ€§èƒ½æ¥è¿‘äºGPT-4ï¼Œå…·æœ‰å¤šè¯­è¨€å¤„ç†èƒ½åŠ›ã€‚æ­¤å¤–Mistralè¿˜æœ‰ä¸€ä¸ªembeddingæ¨¡å‹ï¼Œç”¨äºèšç±»å’Œåˆ†ç±»ã€‚
[Le Chat](https://chat.mistral.ai/)ï¼šç±»ä¼¼äºchatgptï¼Œæ³¨å†Œå³å¯ä½¿ç”¨ã€‚
æœ¬åœ°è¿è¡Œï¼šä½¿ç”¨transformersã€llama.cppã€ollamaç­‰åº“
ä½¿ç”¨APIï¼šåœ¨[La Plateforme](https://console.mistral.ai)ä¸­è®¾ç½®APIå¯†é’¥![Pasted image 20240807205354](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240807205354.png)
# Prompting
å®‰è£…ç›¸å…³åº“ï¼š`pip install mistralai`
## promptæ–‡æœ¬
```Python
prompt = """
    You are a bank customer service bot. 
    Your task is to assess customer intent and categorize customer 
    inquiry after <<<>>> into one of the following predefined categories:
    
    card arrival
    change pin
    exchange rate
    country support 
    cancel transfer
    charge dispute
    
    If the text doesn't fit into any of the above categories, 
    classify it as:
    customer service
    
    You will only respond with the predefined category. 
    Do not provide explanations or notes. 
    
    ###
    Here are some examples:
    
    Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?
    Category: card arrival
    Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.
    Category: exchange rate 
    Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.
    Category: country support
    Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. 
    Category: customer service
    ###
    
    <<<
    Inquiry: {inquiry}
    >>>
    Category:
"""
```
åœ¨è¿™æ®µPromptæ–‡æœ¬ä¸­ï¼Œé¦–å…ˆä½¿ç”¨role-playæ¥ä¸ºæ¨¡å‹æä¾›ä¸€ä¸ªè§’è‰²-é“¶è¡Œå®¢æˆ·æœåŠ¡æœºå™¨äººã€‚å…¶æ¬¡ï¼Œä½¿ç”¨äº†few-shot learningï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ç¬¬ä¸‰ï¼Œä½¿ç”¨å“ˆå¸Œæˆ–è€…è§’æ‹¬å¼§ç­‰æ ‡è¯†ç¬¦æ¥æŒ‡å®šæ–‡æœ¬ä¸åŒéƒ¨åˆ†çš„è¾¹ç•Œã€‚

è¯¥promptæ–‡æœ¬çš„ä»»åŠ¡æ˜¯è¯„ä¼°å®¢æˆ·æ„å›¾å¹¶å¯¹å®¢æˆ·å’¨è¯¢è¿›è¡Œåˆ†ç±»ï¼Œé¦–å…ˆç»™å‡ºä¸€äº›é¢„å…ˆå®šä¹‰çš„ç§ç±»ï¼Œå¹¶æä¾›äº†ä¸€äº›ä¾‹å­ï¼Œé€šè¿‡`format`æ–¹æ³•æ›¿æ¢`inquiry`æ¥è¿›è¡ŒæŸ¥è¯¢ï¼š
```Python
mistral(
    response.format(
        inquiry="I am inquiring about the availability of your cards in the EU"
    )
)
```
## JSONæ ¼å¼æå–ä¿¡æ¯
```Python
prompt = f"""
Extract information from the following medical notes:
{medical_notes}

Return json format with the following JSON schema: 

{{
        "age": {{
            "type": "integer"
        }},
        "gender": {{
            "type": "string",
            "enum": ["male", "female", "other"]
        }},
        "diagnosis": {{
            "type": "string",
            "enum": ["migraine", "diabetes", "arthritis", "acne"]
        }},
        "weight": {{
            "type": "integer"
        }},
        "smoking": {{
            "type": "string",
            "enum": ["yes", "no"]
        }}
}}
"""
```
åœ¨promptä¸­ï¼Œç»™å‡ºä¸€ä¸ªæ¨¡æ¿ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿè¾“å‡ºjsonæ ¼å¼æ–‡æœ¬ã€‚
åœ¨è°ƒç”¨å‡½æ•°æ—¶ï¼Œéœ€è¦å°†`is_json`è®¾å®šä¸ºTrueï¼Œä»¥å¯ç”¨JSONæ¨¡å¼ã€‚
```Python
response = mistral(prompt, is_json=True)
print(response)
```
ä¸‹é¢æ˜¯è¾…åŠ©å‡½æ•°`mistral`çš„å®šä¹‰ï¼š
```Python
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage Â 
def mistral(user_message,
Â  Â  Â  Â  Â  Â  model="mistral-small-latest",
Â  Â  Â  Â  Â  Â  is_json=False):
Â  Â  client = MistralClient(api_key=os.getenv("MISTRAL_API_KEY"))
Â  Â  messages = [ChatMessage(role="user", content=user_message)]
Â  Â  if is_json:
Â  Â  Â  Â  chat_response = client.chat(
Â  Â  Â  Â  Â  Â  model=model,
Â  Â  Â  Â  Â  Â  messages=messages,
Â  Â  Â  Â  Â  Â  response_format={"type": "json_object"})
Â  Â  else:
Â  Â  Â  Â  chat_response = client.chat(
Â  Â  Â  Â  Â  Â  model=model,
Â  Â  Â  Â  Â  Â  messages=messages)
Â  Â  return chat_response.choices[0].message.content
```
é¦–å…ˆå®šä¹‰`mistral`çš„clientï¼Œä½¿ç”¨api keyæ¥è¿è¡Œï¼Œå®šä¹‰ä¸æ¨¡å‹å¯¹è¯çš„`message`ã€‚å¦‚æœéœ€è¦è¿”å›Jsonæ ¼å¼ï¼Œåˆ™éœ€è¦æ·»åŠ `response_format={"type": "json_object"}`ã€‚
# Model selecting
Mistralæä¾›äº”ä¸ªæ¨¡å‹ï¼š

| Model          | Endpoint              |
| -------------- | --------------------- |
| Mistral 7B     | open-mistral-7b       |
| Mixtral 8x7B   | open-mixtral-8x7b     |
| Mistral Small  | mistral-small-latest  |
| Mistral Medium | mistral-medium-latest |
| Mistral Large  | mistral-large-latest  |
## Mistral small
mistral smallæ¨¡å‹é€‚åˆç”¨äºç®€å•ä»»åŠ¡ï¼Œå¿«é€Ÿæ¨ç†ï¼Œè´¹ç”¨è¾ƒä½ï¼Œæ¯”å¦‚ç”¨äºåƒåœ¾é‚®ä»¶åˆ†ç±»ï¼š
```Python
prompt = """
Classify the following email to determine if it is spam or not.
Only respond with the exact text "Spam" or "Not Spam". 

# Email:
ğŸ‰ Urgent! You've Won a $1,000,000 Cash Prize! 
ğŸ’° To claim your prize, please click on the link below: 
https://bit.ly/claim-your-prize
"""
mistral(prompt, model="mistral-small-latest")
# 'Spam'
```
## Mistral medium
ä¸­å‹æ¨¡å‹é€‚ç”¨äºä¸­çº§ä»»åŠ¡ï¼Œå¦‚è¯­è¨€è½¬æ¢ã€‚æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡æ’°å†™æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ ¹æ®è´­ä¹°ä¿¡æ¯æ’°å†™å®¢æˆ·æœåŠ¡ç”µå­é‚®ä»¶ï¼‰ã€‚
```Python
prompt = """
Compose a welcome email for new customers who have just made 
their first purchase with your product. 
Start by expressing your gratitude for their business, 
and then convey your excitement for having them as a customer. 
Include relevant details about their recent order. 
Sign the email with "The Fun Shop Team".

Order details:
- Customer name: Anna
- Product: hat 
- Estimate date of delivery: Feb. 25, 2024
- Return policy: 30 days
"""
response_medium = mistral(prompt, model="mistral-medium-latest")
```
## Mistral large
å¤§å‹æ¨¡å‹é€‚åˆéœ€è¦é«˜çº§åŠŸèƒ½ã€é«˜çº§æ¨ç†çš„å¤æ‚ä»»åŠ¡ã€‚
```Python
prompt = """
Calculate the difference in payment dates between the two \
customers whose payment amounts are closest to each other \
in the following dataset. Do not write code.

# dataset: 
'{
  "transaction_id":{"0":"T1001","1":"T1002","2":"T1003","3":"T1004","4":"T1005"},
    "customer_id":{"0":"C001","1":"C002","2":"C003","3":"C002","4":"C001"},
    "payment_amount":{"0":125.5,"1":89.99,"2":120.0,"3":54.3,"4":210.2},
"payment_date":{"0":"2021-10-05","1":"2021-10-06","2":"2021-10-07","3":"2021-10-05","4":"2021-10-08"},
    "payment_status":{"0":"Paid","1":"Unpaid","2":"Paid","3":"Paid","4":"Pending"}
}'
"""
response_small = mistral(prompt, model="mistral-small-latest")
print(response_small)# wrong answer
response_large = mistral(prompt, model="mistral-large-latest")
print(response_large)# right answer
```
å°å‹æ¨¡å‹çš„å›ç­”é”™è¯¯ï¼Œä½†ç”±äºæ¨¡å‹çš„è¾“å‡ºæ˜¯æ¦‚ç‡æ€§çš„ï¼Œæ‰€ä»¥åœ¨å¤šæ¬¡è¿è¡Œåå¯èƒ½ä¼šè¾“å‡ºæ­£ç¡®ç»“æœã€‚
å¤§å‹æ¨¡å‹å°†é—®é¢˜åˆ†ä¸ºå¤šä¸ªæ­¥éª¤ï¼Œå¹¶ç»™å‡ºæ­£ç¡®çš„ç­”æ¡ˆã€‚

Mistral largeçš„ä»£ç å®ç°ä¹Ÿæ˜¯å¯è¡Œçš„ã€‚

Mistral largeèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå¤šç§è¯­è¨€çš„æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨ Mistral æ¨¡å‹è¿›è¡Œæ›´å¤šçš„å·¥ä½œï¼Œè€Œä¸ä»…ä»…æ˜¯å°†ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€‚
# Function calling
å‡½æ•°è°ƒç”¨å…è®¸Mistralæ¨¡å‹è¿æ¥å¤–éƒ¨å·¥å…·ï¼Œå¯ä»¥æ„å»ºç‰¹å®šçš„ç”¨ä¾‹å’Œå®é™…é—®é¢˜çš„åº”ç”¨ç¨‹åºã€‚
å‡½æ•°è°ƒç”¨æ­¥éª¤ï¼š
1. ç”¨æˆ·è‡ªå®šä¹‰å·¥å…·toolså¹¶ä½¿ç”¨æŸ¥è¯¢ã€‚toolå¯ä»¥æ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„å‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ˜¯å¤–éƒ¨çš„APIã€‚
2. ç”±æ¨¡å‹ç”Ÿæˆå‡½æ•°å‚æ•°ã€‚æ¨¡å‹æ ¹æ®toolså’Œç”¨æˆ·æŸ¥è¯¢ï¼Œå¯ä»¥å†³å®šåº”è¯¥ä½¿ç”¨toolsä¸­çš„å“ªä¸ªå‡½æ•°
3. ç”¨æˆ·æ‰§è¡Œå‡½æ•°ï¼Œä»å‡½æ•°ä¸­å¾—åˆ°ç»“æœã€‚
4. æ¨¡å‹æ ¹æ®ç°æœ‰çš„èµ„æ–™ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
![Pasted image 20240808160115](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240808160115.png)
# RAG from scratch
RAGï¼ˆRetrieval Augmented Generationï¼‰æ˜¯ä¸€ä¸ªaiæ¡†æ¶ï¼Œç»“åˆäº†å¤§é¢„è¨€æ¨¡å‹çš„ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œæœ‰åŠ©äºå›ç­”é—®é¢˜æˆ–ç”Ÿæˆä¸å¤–éƒ¨åªæ˜¯ç›¸å…³çš„å†…å®¹ã€‚
å½“ç”¨æˆ·å°±å†…éƒ¨æ–‡ä»¶æˆ–çŸ¥è¯†åº“æå‡ºé—®é¢˜æ—¶ï¼Œæˆ‘ä»¬ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚å…¶ä¸­ï¼Œæ‰€æœ‰çš„æ–‡æœ¬å‰å¦‚éƒ½å­˜å‚¨åœ¨ä¸€ä¸ªå‘é‡å­˜å‚¨åŒºä¸­ï¼Œè¿™ä¸€æ­¥éª¤ç§°ä¸ºretrievalã€‚åœ¨promtä¸­ï¼Œä½¿ç”¨æŸ¥è¯¢å’Œç›¸å…³ä¿¡æ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç›¸å…³è¯­å¢ƒç”Ÿæˆè¾“å‡ºç»“æœï¼Œè¯¥æ­¥éª¤æˆä¸ºgenerationã€‚
## RAGä¾‹å­
é¦–å…ˆä½¿ç”¨`BeautifulSoup`æ¥è·å–æ–‡ç« ï¼Œç„¶åå°†æ–‡ç« åˆ†å—ä»¥ä¾¿æ›´æœ‰æ•ˆè¯†åˆ«å’Œæ£€ç´¢æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚
```Python
import requests
from bs4 import BeautifulSoup
import re

response = requests.get(
    "https://www.deeplearning.ai/the-batch/a-roadmap-explores-how-ai-can-detect-and-mitigate-greenhouse-gases/"
)
html_doc = response.text
soup = BeautifulSoup(html_doc, "html.parser")
tag = soup.find("div", re.compile("^prose--styled"))
text = tag.text
print(text)
chunk_size = 512
chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]
```
å¾—åˆ°æ–‡æœ¬å—åï¼Œéœ€è¦å¯¹å…¶è¿›è¡Œembeddingï¼š
```Python
import os
from mistralai.client import MistralClient
import numpy as np
def get_text_embedding(txt):
Â  Â  client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)
Â  Â  embeddings_batch_response = client.embeddings(model="mistral-embed", input=txt)
Â  Â  return embeddings_batch_response.data[0].embedding

text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])
```
å¾—åˆ°åµŒå…¥åï¼Œé€šå¸¸çš„åšæ³•æ˜¯å°†ä»–ä»¬å­˜å‚¨åœ¨é€‚é‡æ•°æ®åº“ä¸­ï¼Œä»¥ä¾¿é«˜æ•ˆå¤„ç†å’Œæ£€ç´¢ã€‚
```Python
import faiss
d = text_embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(text_embeddings)
```
å½“ç”¨æˆ·æå‡ºé—®é¢˜æ—¶ï¼Œé—®é¢˜ä¹Ÿéœ€è¦åšåŒæ ·çš„embeddingã€‚
æ¥ä¸‹æ¥ï¼Œä»çŸ¢é‡æ•°æ®åº“ä¸­æ£€ç´¢æ–‡æœ¬å—ï¼š
```Python
D, I = index.search(question_embeddings, k=2)
```
è¯¥å‡½æ•°è¿”å›çš„æ˜¯kä¸ªåœ¨çŸ¢é‡æ•°æ®åº“ä¸­ä¸é—®é¢˜æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—ã€‚
å°†æ£€ç´¢åˆ°çš„æ–‡æœ¬å—å’Œé—®é¢˜ç»“åˆï¼Œä½œä¸ºpromptä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
```Python
prompt = f"""
Context information is below.
---------------------
{retrieved_chunk}
---------------------
Given the context information and not prior knowledge, answer the query.
Query: {question}
Answer:
"""
response = mistral(prompt)
print(response)
```
## RAG + Function calling
å®šä¹‰ä¸€ä¸ªå‡½æ•°`qa_with_context`
```Python
def qa_with_context(text, question, chunk_size=512):
Â  Â  # split document into chunks
Â  Â  chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]
Â  Â  # load into a vector database
Â  Â  text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])
Â  Â  d = text_embeddings.shape[1]
Â  Â  index = faiss.IndexFlatL2(d)
Â  Â  index.add(text_embeddings)
Â  Â  # create embeddings for a question
Â  Â  question_embeddings = np.array([get_text_embedding(question)])
Â  Â  # retrieve similar chunks from the vector database
Â  Â  D, I = index.search(question_embeddings, k=2)
Â  Â  retrieved_chunk = [chunks[i] for i in I.tolist()[0]]
Â  Â  # generate response based on the retrieve relevant text chunks

Â  Â  prompt = f"""
Â  Â  Context information is below.
Â  Â  ---------------------
Â  Â  {retrieved_chunk}
Â  Â  ---------------------
Â  Â  Given the context information and not prior knowledge, answer the query.
Â  Â  Query: {question}
Â  Â  Answer:
Â  Â  """
Â  Â  response = mistral(prompt)
Â  Â  return response
```
å°†è¯¥å‡½æ•°ç¼–å…¥åˆ°è¯å…¸ä¸­ï¼Œç”¨äºtoolsæ„å»ºã€‚
```Python
import functools
names_to_functions = {"qa_with_context": functools.partial(qa_with_context, text=text)}
```
ä½¿ç”¨ä¸€ä¸ªjsonæ¨¡å¼æ¥æ¦‚æ‹¬åŠŸèƒ½è§„æ ¼ï¼Œå‘Šè¯‰æ¨¡å‹toolsçš„åŠŸèƒ½ï¼š
```Python
tools = [
    {
        "type": "function",
        "function": {
            "name": "qa_with_context",
            "description": "Answer user question by retrieving relevant context",
            "parameters": {
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "user question",
                    }
                },
                "required": ["question"],
            },
        },
    },
]
```
å°†ç”¨æˆ·é—®é¢˜å’Œtoolä¼ å…¥ç»™æ¨¡å‹ï¼š
```Python
question = """
What are the ways AI can mitigate climate change in transportation?
"""
client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)
response = client.chat(
    model="mistral-large-latest",
    messages=[ChatMessage(role="user", content=question)],
    tools=tools,
    tool_choice="any",
)
response
```
å…¶ä¸­`tool_choice`æœ‰ä»¥ä¸‹é€‰é¡¹ï¼š
1. `any`ï¼šå¼ºåˆ¶æ¨¡å‹ä½¿ç”¨toolsä¸­çš„ä¸€ç§
2. `auto`ï¼šè®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ï¼Œå¯ä»¥ä½¿ç”¨toolsæˆ–è€…ä¸ä½¿ç”¨
3. `none`ï¼šç¦æ­¢æ¨¡å‹ä½¿ç”¨toolsã€‚
# Chatbot
åœ¨æœ¬èŠ‚ä¸­å°†æ„å»ºä¸€ä¸ªå¯è§†åŒ–çš„å¯¹è¯çª—å£ã€‚
é¦–å…ˆå¼•å…¥`panel`ï¼ŒPanel æ˜¯ä¸€ä¸ªå¼€æº Python åº“ï¼Œå¯ç”¨äºåˆ›å»ºä»ªè¡¨ç›˜å’Œåº”ç”¨ç¨‹åºã€‚`pn.extension()`ç”¨äºåŠ è½½è‡ªå®šä¹‰Javascriptå’ŒCSSæ‰©å±•ã€‚
```Python
import panel as pn
pn.extension()
```
æ›´æ–°`mistral`å‡½æ•°ï¼Œæ–°å¢`user`å’Œ`chat_interface`ä¸¤ä¸ªå‚æ•°ï¼š
```Python
def run_mistral(contents, user, chat_interface):
    client = MistralClient(api_key=api_key, endpoint=dlai_endpoint)
    messages = [ChatMessage(role="user", content=contents)]
    chat_response = client.chat(
        model="mistral-large-latest", 
        messages=messages)
    return chat_response.choices[0].message.content

chat_interface = pn.chat.ChatInterface(
    callback=run_mistral, 
    callback_user="Mistral"
)
chat_interface
```
ä½¿ç”¨`pn.chat.ChatInterface`å®šä¹‰äº†ä¸€ä¸ªèŠå¤©ç•Œé¢è½¯ä»¶ï¼Œè¯¥éƒ¨ä»¶è´Ÿè´£å¤„ç†èŠå¤©æœºå™¨äººçš„æ‰€æœ‰ç”¨æˆ·ç•Œé¢å’Œé€»è¾‘ã€‚
## ä¸Šä¼ æ–‡ä»¶
å®šä¹‰ä¸€ä¸ªæ–‡ä»¶è¾“å…¥widgetä»¥ä¾¿ä¸Šä¼ æ–‡ä»¶ï¼Œåœ¨headerä¸­å°†è¯¥éƒ¨ä»¶åŠ å…¥ã€‚
```Python
file_input = pn.widgets.FileInput(accept=".txt", value="", height=50)

chat_interface = pn.chat.ChatInterface(
    callback=answer_question,
    callback_user="Mistral",
    header=pn.Row(file_input, "### Upload a text file to chat with it!"),
)
chat_interface.send(
    "Send a message to get a reply from Mistral!", 
    user="System", 
    respond=False
)
chat_interface
```