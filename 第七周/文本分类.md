# 加载数据集
加载IMDB数据集：
```Python
from datasets import load_dataset
imdb = load_dataset("imdb")
```
数据集格式：
```
imdb:
- train
- test:
    - 0:{'text':str, 'label': 0}
```
# 预处理
使用distilbert分词器来预处理文本：
```Python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)
tokenized_imdb = imdb.map(preprocess_function, batched=True)
```
为了将`preprocess_function`用于整个数据集，需要使用`map`方法将数据集映射到函数上。
使用`DataCollatorWithPadding`创建batch示例。
```Python
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
# Evaluate
使用Evaluate库快速加载评估方法，加载`accuracy`指标：
```Python
import evaluate
accuracy = evaluate.load("accuracy")
```
`accuracy`有一个方法`compute`，传入prediction和label便可以计算：
```Python
import numpy as np
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)
```
# Train
训练之前，构建label与id的映射：
```Python
id2label = {0: "NEGATIVE", 1: "POSITIVE"}
label2id = {"NEGATIVE": 0, "POSITIVE": 1}
```
`AutoModelForSequenceClassification`用于处理序列分类任务。
**注意**：从模型的配置文件加载模型不会加载模型权重，需要使用`from_pretrained`来加载模型权重。
```Python
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert/distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
)
```
后续步骤：
1. 定义超参数以及指定模型输出路径
2. 将训练的参数、模型、数据集、tokenizer等传给trainer
3. 调用`trainer.train()`来微调模型
```Python
training_args = TrainingArguments(...)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
trainer.train()
```
# Inference
使用`pipeline`来进行推理，pipeline接受的参数有四个：
1.  `task`：指定要执行的任务类型，例如 `sentiment-analysis`、`question-answering` 等。
2. `model`：指定要使用的模型名称或路径。如果不指定，将使用默认模型。
3. `tokenizer`：指定要使用的分词器。如果不指定，将根据模型自动选择。
4. `device`：指定是否使用 GPU（例如 `device=0` 表示使用第一个 GPU），默认为 CPU。
```Python
from transformers import pipeline
classifier = pipeline("sentiment-analysis", model="stevhliu/my_awesome_model")
classifier(text)
```
也可以调用模型来推理：
```Python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_model")
inputs = tokenizer(text, return_tensors="pt")
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("stevhliu/my_awesome_model")
with torch.no_grad():
    logits = model(**inputs).logits
predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
```