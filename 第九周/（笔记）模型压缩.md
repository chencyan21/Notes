# 概述
端侧设备存在资源限制，模型计算量大，存储成本高，复杂度高，需要模型压缩，对于云端设备也可以进行模型压缩。深度神经网络模型存在冗余性，利用网络模型参数以及网络结构的冗余性，对深度学习模型进行重构、简化并且加速的技术，在不影响原任务完成度的情况下，得到参数更少以及结构更加精简的模型。
主流的模型压缩方法：
1. 剪枝：修剪不重要的网络连接
2. 量化：将连续型数据量化为低位宽离散数据
3. 知识蒸馏：大模型指导小模型学习
4. 低秩分解：通过低秩矩阵近似原矩阵
5. 轻量化网络：使用轻量化卷积核代替传统卷积
6. 网络结构搜索：自动化地设计优异网络模型
## 剪枝
模型中并不是所有的参数对模型的输出都起着至关重要的作用，剪枝利用模型的冗余性去减少模型规模进一步提高推理速度。剪枝根据连接的强弱去除网络中一些不重要的连接。
![Pasted image 20240822171751](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240822171751.png)
经过剪枝后，模型的稀疏度增加，模型的表达能力可能会有一定的下降。
通常的剪枝流程：首先对模型进行预训练，根据一定的评估标准，对模型进行剪枝，然后使用微调恢复模型的准确度。
## 量化
将连续取值的浮点型权重量化为有限多个离散值的过程，通过减少模型精度的方法来减小模型规模。可以直接对原有的浮点型模型进行量化，不需要开发新的模型架构，也不用对模型进行重新训练，只会带来微笑的准确率损失。
根据原始数据是否分布均匀，可以将量化分为线性量化和非线性量化。非线性量化的计算复杂度比较高，通常采用线性量化。
![Pasted image 20240822191410](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240822191410.png)

## 知识蒸馏
知识蒸馏是通过训练好的复杂的教师网络在相同的数据下指导小型模型学习的一种方法。本质上是将大模型的泛化能力迁移给小模型。
将教师网络输出的类概率作为学生模型学习的目标，类概率也被称为**软目标**，对应的真实数据标签也被称为硬目标。
通过软目标代替以往的硬目标进行损失计算、网络更新，软目标相交于硬目标不仅具有正标签，还具有负标签的信息。学生不仅学习标准答案，而且还学习老师提供答案。
标准答案往往只有正确的信息，学生模型学习之后只知道正确的而不知道错误的情况，而老师不仅给学习提供了正确的答案还能帮助学生分析错误的因素。
![Pasted image 20240823204225](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823204225.png)

## 低秩分解
分为SVD分解、CP分解、Tucker分解和Tensor Train分解。低秩分解主要集中在卷积运算中，利用卷积核矩阵的低秩特性，通过一些低秩的基础张量去近似原始的大规模卷积核矩阵。![Pasted image 20240823205123](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823205123.png)
## 轻量化网络
将深度神经网络中的标准卷积核用小尺寸的卷积核代替。
示例：
- SqueezeNet
- MobileNet
- ShuffleNet
## 网络结构搜索
首先定义一个搜索空间，通过搜索策略去采样得到一个网络结构，然后对网络结构进行性能评估并反馈给搜索策略，重复以上步骤得到一个表现最佳的网络结构。![Pasted image 20240823211928](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823211928.png)

# 知识蒸馏
知识蒸馏：通过教师模型指导学生模型训练。教师模型是一个提前训练好的复杂模型，学生模型是一个规模较小的模型。
## 知识种类
输出特征知识：教师模型输出的相关内容。对于分类模型，可以是logits或者软目标。
中间特征知识：教师模型的中间层上输出的相关信息，可以将中间特征知识看作是问题的求解过程。
关系特征知识：教师模型不同层和不同样本数据之间的关系知识。可以比作求解问题的方法。
结构特征知识：包含以上提到的所有知识，还包括教师模型的区域特征分布等知识。
![Pasted image 20240823215058](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823215058.png)
## 蒸馏机制
根据教师网络和学生网络是否一起更新，可以将知识蒸馏分为：
- **离线蒸馏**：将一个预训练好的教师模型的只是迁移到学生网络中，包含以下两个阶段，在蒸馏前，教师网络在训练集上进行训练，第二阶段，教师网络通过logits层或者中间层提取知识信息引导学生网络进行训练。实现比较简单，形式上通常是单向的知识迁移。
- **在线蒸馏**：教师和学生一起学习。教师模型和学生模型同时更新并且整个知识蒸馏框架是端到端可训练的。
- **自蒸馏**：教师和学生模型使用相同的网络。分为两类：第一类是使用不同样本信息进行相互蒸馏，第二类是单个网络的网络层间进行自蒸馏，通常做法是使用深层网络的特征去指导浅层网络的学习。
## 师生网络架构
学生网络一般是：
1. 教师网络的简化版本，具有较少的层和每层中较少的信道。
2. 教师网络的量化版本，其中网络的结构被保留。
3. 具有高效基本操作的小型网络。
4. 具有优化的整体网络结构的小型网络。
5. 与教师相同的网络。
## 蒸馏算法
### 对抗蒸馏
在对抗性学习中，对抗网络中的鉴别器用来估计样本来自训练数据分布的概率，而生成器试图使用生成的数据样本来欺骗鉴别器。受此启发，已经出现了许多基于对抗的知识蒸馏方法，以使教师和学生网络能够更好地理解真实的数据分布。
![Pasted image 20240823222232](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222232.png)
### 多教师蒸馏
不同的教师架构可以为学生网络提供不同有用的知识。在训练学生网络期间，多个教师网络可以单独地，也可以整体地用于蒸馏。为了传递来自多个教师的知识，最简单的方法是使用来自所有教师的平均响应作为监督信号。![Pasted image 20240823222242](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222242.png)
### 交叉模式蒸馏
在训练或测试期间，某些数据或标签可能不可用。因此，在不同的模型之间传递知识是很重要的。然而，当模型存在差异时，跨模型知识蒸馏是一项具有挑战性的研究，例如，当不同模式之间缺乏配对的样本时。
![Pasted image 20240823222332](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222332.png)
### 基于图形的蒸馏
基于图的蒸馏方法的主要思想是：
1. 用图作为教师知识的载体；
2. 用图来控制教师知识的信息传递。
![Pasted image 20240823222350](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222350.png)
### 无数据蒸馏
为了克服由隐私、合法性、安全性和保密性问题等原因引起的不可用数据的问题，出现了一些无数据知识蒸馏的方法。无数据蒸馏中的合成数据通常是从预训练教师模型的特征表示中生成的。![Pasted image 20240823222451](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222451.png)
### 量化蒸馏
一个大的高精度的教师网络将知识传递给一个小的低精度的学生网络。为了确保小的学生网络精确地模仿大的教师网络，首先在特征图上量化教师网络，然后将知识从量化的教师转移到量化的学生网络。![Pasted image 20240823222519](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222519.png)
## 蒸馏流程
step1：训练Teacher模型。
step2：利用高温T产生Soft-target，用T=1产生Hard-target。
step3：利用{高温T,Soft-target}和{T=1，Hard-target}同时训练Student模型
step4：设置T=l，Student模型线上做推理。
![Pasted image 20240823222641](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222641.png)![Pasted image 20240823222730](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823222730.png)
# 剪枝
过参数化：过参数化主要是指在训练阶段，在数学上需要进行大量的微分求解，去捕捉数据中微小的变化信息，一旦完成迭代式的训练之后，网络模型在推理的时候就不需要这么多参数。而剪枝算法正是基于过参数化的理论基础提出来的。剪枝算法核心思想就是减少网络模型中参数量和计算量，同时尽量保证模型的性能不受影响。
## 剪枝步骤
对模型进行剪枝三种常见做法：
1. 训练一个模型，对模型进行剪枝，对剪枝后模型进行微调（最常见）
2. 在模型训练过程中进行剪枝，对剪枝后模型进行微调
3. 进行剪枝，从头训练剪枝后模型
训练：是对网络模型进行训练。
剪枝：在这里面可以进行如细粒度剪枝、向量剪枝、核剪枝、滤波器剪枝等各种不同的剪枝算法。
微调：微调是恢复被剪枝操作影响的模型表达能力的必要步骤。
## 结构化剪枝和非结构化剪枝
![Pasted image 20240823225546](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823225546.png)
从左到右，剪枝粒度依次递增。
非结构化剪枝主要是对一些独立的权重或者神经元在或者一些神经元的连接接进行剪枝，就是随机的剪，是粒度最小的剪枝。预定义一个阈值，低于这个阈值的权重被剪去，高于的被保留。另一种方法是使用一个拼接函数来屏蔽权重：$$\Delta w=-\eta\frac{\partial L}{\partial(h(w)w)}$$里面的h(w)逐渐将不必要的权重减少到0：![Pasted image 20240823231507](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823231507.png)
右边的这三个图是结构化剪枝，结构化的剪枝是有规律、有顺序的。对神经网络，或者计算图进行剪枝，几个比较经典的就是对layeri进行剪枝，对channel进行剪枝，对Filteri进行剪枝，剪枝粒度依次增大。
## 静态剪枝与动态剪枝
![Pasted image 20240823231625](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240823231625.png)
静态剪枝在训练后和推理前进行剪枝。在推理过程中，不需要对网络进行额外的剪枝。
静态剪枝通常包括三个部分：
1. 剪枝参数的选择：
2. 剪枝的方法；
3. 选择性微调或再训练。
静态剪枝存储成本低，适用于资源有限的边缘设备。但存在下列问题：
1. 通道的删除是永久性的，对于一些较为复杂的输入数据，可能无法达到很好的精度。
2. 需要精心设计要剪的部分，不然容易造成计算资源的浪费。
3. 神经元的重要性并不是静态的，而且神经元的重要性很大程度上依赖于输入数据，静态的剪枝很容易降低模型的推理性能。
动态剪枝：网络中有一些奇怪的权重，他们在某些迭代中作用不大，但在其他的迭代却很重要。动态剪枝就是通过动态的恢复权重来得到更好的网络性能。动态剪枝在运行时才决定哪些层、通道或滤波器不会参与进一步的活动。
动态剪枝能够提高卷积神经网络的表达能力，性能更好，但存在以下问题：
1. 之前有方法通过强化学习来实现动态剪枝，但在训练过程中要消耗非常多的运算资源。
2. 很多动态剪枝的方法都是通过强化学习的方式来实现的，但是阀门的开关是不可微的，也就是说，梯度下降法在这里是用不了的。
3. 存储成本高，不适用于资源有限的边缘设备。
## 硬剪枝与软剪枝
硬剪枝：在每个epoch后会将卷积核直接剪掉，被剪掉的卷积核在下一个epoch中不会再出现。存在的问题：
1. 模型性能降低；
2. 依赖预先训练的模型。
软剪枝：相比较硬剪枝，软剪枝剪枝后进行训练时，上一个epoch中被剪掉的卷积核在当前epoch训练时仍参与迭代，只是将其参数置为0，因此那些卷积核不会被直接丢弃。
软剪枝步骤：
1. 滤波器选择：使用L2范数来评估每个滤波器的重要性。具有较小L2范数的滤波器的卷积结果会导致相对较低的激活值，从而对网络模型的最终的预测结果有较小的影响
2. 滤波器剪枝：将所选滤波器的值设置为零，在接下来的训练阶段，这些滤波器仍然可以被更新，以保持模型的高性能。在滤波器剪枝步骤中，可以同时修剪所有加权层。此外，要对所有加权层使用相同的剪枝率
3. 重建：训练一个轮来重构修剪后的滤波器。修剪滤波器通过反向传播被更新为非零，经过软剪枝的模型可以具有与原始模型相当的性能。
4. 获得紧凑模型：重复以上3个步骤。

# 量化
模型量化是指将神经网络模型中的连续取值的权重或激活值近似为有限多个离散值的过程。
优势：
1. 压缩参数
2. 提升速度
3. 降低内存占用
劣势：
1. 模型精度下降
量化：$$Q= clamp(round(\frac{r}{s}))$$
反量化：$$r=s\times Q$$
其中：r为浮点值，Q为整型值。
## 量化分类
分为线性量化和非线性量化，因为量化过程中引入了四舍五入和clamp操作，所以反量化回去的值和原始的浮点值并不相同。![Pasted image 20240824101538](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240824101538.png)
在非线性量化中，网络中的值分布是不均匀的，量化值之间的间隔不固定。非线性量化可以更好的捕获分布相关的信息，数据多的地方量化间隔小，量化精度高。因此非线性量化的效果理论上比线性量化更好。但是通用硬件加速比较困难。因此线性量化是更常用的方法。
### 对称量化与非对称量化
根据浮点值的零点是否映射到量化值的零点，可以将量化分为对称量化和非对称量化。![Pasted image 20240824101753](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240824101753.png)
非对称量化有一个额外的参数z来调整零点的映射，该参数通常称为零点。非对称量化的效果通常比对称量化的效果更好，但是需要额外的存储，且推理的时候需要计算零点的内容。

### 逐层量化和逐通道量化
根据量化参数（缩放因子s，零点z）的共享范围，可以将量化分为逐层量化和逐通道量化
![Pasted image 20240824102151](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240824102151.png)
**逐层量化**：在逐层量化中，每个网络层中的所有filter共享相同的量化参数。浮点值范围由当前层所有的filter确定，对每层中所有的filter采用相同的范围，对应的缩放因子和零点也是相同的。实现简单但是效果不是很好，因为不同filter的范围可能会有很大差异。
**逐通道量化**：逐通道量化是一种更细粒度的量化方法。为每层的各个filter单独地计算需要表示的范围以及量化参数。更好保留每个filter的信息，效果较好。
### 统一精度量化和混合精度量化
根据网络中量化位宽的不同，可以将量化分为统一精度量化和混合精度量化。
**统一精度**：所有量化的网络层均采用相同的位宽（相同的整型类型）。方法比较简单，不需要考虑不同层对量化的敏感度。但是采用这种方法，在对网络进行量化或量化到较低精度时，可能会引起网络准确率的显著下降。
**混合精度**：不同的网络层可以量化到不同的位宽。核心思想是将不适合量化的层保留在较高精度，适合量化的层进行更加激进的量化。使网络整体处于较低位宽，并尽量缓解网络准确率的下降。混合精度量化中需要解决的问题与统一精度类似。

## 量化方式
分为训练后量化和量化感知训练
### 训练后量化
训练后量化直接对已训练完成的模型进行量化，无需微调或训练过程，因此训练后量化的开销较小。训练后量化只需要一小部分数据驱动量化，能很好地应用于数据敏感的场景。但是训练后量化的模型精度下降可能要高于量化感知训练。训练后量化可以分为权重量化和全量化两种。
**权重量化**：仅对模型的权重进行量化操作，以整型形式存储模型权重，可以压缩模型的大小。在推理阶段首先将量化的权重反量化为浮点形式，推理过程仍然为浮点计算，无法加速推理过程。
**全量化**：对模型权重和激活值都进行量化，不仅可以压缩模型大小，减少推理过程的内存占用，而且因为激活值和权重都为整型数据，可以使用高效的整型运算单元加速推理过程。全量化可以分为两种形式：静态量化和动态量化。
- 静态量化：离线计算权重与激活的量化参数
- 动态量化：推理时动态计算激活的量化参数。虽然效果更好，但是会给推理带来额外的开销。
### 量化感知训练
量化感知训练在训练好的模型上插入伪量化算子，即对数值量化然后反量化，模拟量化产生的误差。然后在训练数据集更新权重并调整对应的量化参数，或者直接将量化参数作为可学习的参数在反向传播中更新。这种方法得到的量化模型精度较高，但是因为需要训练过程，因此开销较大，而且对于数据的要求相对于训练后量化也更高。
![Pasted image 20240824110636](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240824110636.png)
在计算梯度时如何处理量化操作中的不可微分的部分（round操作）：一种传统的方法是使用staright through estimator（STE）将伪量化算子的梯度设置为1，即输入的梯度等于输出的梯度。
另一种方法是校准方法，校准是调整与确定量化参数的过程。![Pasted image 20240824112015](https://cyan-1305222096.cos.ap-nanjing.myqcloud.com/Pasted%20image%2020240824112015.png)图中b是整型类型的位宽。因为是对称量化，只用正半轴上的值计算缩放因子。因为量化位宽往往是提前确定好的，因此确定缩放因子就是确定我们想要表示的浮点值的范围。
global：最简单的阈值选择方法，直接指定一个全局的值，作为所有网络层的量化阈值。由于每层间值的范围存在差距，因此这种方法很难找到适合所有层的量化阈值。
max：一种简单阈值选择方法，将浮点值的最大绝对值作为量化阈值，这种方法能够表示整个浮点值的范围，但是可能无法充分利用整型值的范围。
percentile：通过分位数确定量化阈值。网络中的值往往不是均匀分布的，大部分是中间多两边少的“钟”型分布，此外还可能存在一些离群点。通过分位数确定阈值，虽然损失了部分数值的信息，但量化效果可能会更好。
mse：最小化量化前后数值之间的差距。通过选择多个候选阈值，记录不同阈值下对浮点值模拟量化的结果，并计算结果与原始的浮点值之间的均方误差，选择使均方误差最小的值作为最终的阈值。
KL-divergence：测试不同threshold下模拟量化值的分布与原始值分布之间的KL散度，选择使KL散度最小的值作为最终threshold。